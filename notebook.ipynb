{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Métricas textuais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de arquivos: ['documents\\\\A9_-_EACL23_Incorporating_context_into_subword_vocabularies.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"documents\"\n",
    "files_list = [os.path.join(path, file_name) for file_name in os.listdir(path)]\n",
    "\n",
    "print(\"Lista de arquivos:\", files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install pymupdf spacy chardet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import chardet\n",
    "\n",
    "# Carrega o modelo de NLP para português\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            for page in pdf:\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler PDF {file_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def remove_references(text):\n",
    "    keywords = [\"Referências\", \"References\"]\n",
    "    last_pos = -1\n",
    "    for word in keywords:\n",
    "        pos = text.rfind(word)\n",
    "        if pos > last_pos:\n",
    "            last_pos = pos\n",
    "    if last_pos != -1:\n",
    "        return text[:last_pos]\n",
    "    return text\n",
    "\n",
    "def load_documents(files):\n",
    "    return [extract_text_from_pdf(file_path) for file_path in files]\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc if not token.is_stop]\n",
    "        cleaned_texts.append(\" \".join(tokens))\n",
    "    return cleaned_texts\n",
    "\n",
    "def count_sentences(doc):\n",
    "    return len(list(doc.sents))\n",
    "\n",
    "def count_tokens(doc):\n",
    "    return [token.text.lower() for token in doc if token.is_alpha]\n",
    "\n",
    "def count_pos_tags(doc):\n",
    "    # Substantivos\n",
    "    num_nouns = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
    "    # Verbos\n",
    "    num_verbs = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "    # Preposições\n",
    "    num_adpositions = sum(1 for token in doc if token.pos_ == \"ADP\")\n",
    "    return num_nouns, num_verbs, num_adpositions\n",
    "\n",
    "def get_lemmas(doc):\n",
    "    tokens = [token for token in doc if not token.is_space]\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "def compute_token_stats(tokens, num_docs):\n",
    "    total_tokens = len(tokens)\n",
    "    avg_tokens = total_tokens / num_docs if num_docs else 0\n",
    "    token_freq = Counter(tokens)\n",
    "    top_10 = token_freq.most_common(10)\n",
    "    down_10 = token_freq.most_common()[-10:]\n",
    "    return total_tokens, avg_tokens, top_10, down_10\n",
    "\n",
    "\n",
    "\n",
    "def get_doc_statistics(texts):\n",
    "    total_sentences = 0\n",
    "    total_tokens_list = []\n",
    "    total_nouns = total_verbs = total_preps = 0\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "\n",
    "        total_sentences += count_sentences(doc)\n",
    "\n",
    "        tokens = count_tokens(doc)\n",
    "        total_tokens_list.extend(tokens)\n",
    "\n",
    "        nouns, verbs, preps = count_pos_tags(doc)\n",
    "        total_nouns += nouns\n",
    "        total_verbs += verbs\n",
    "        total_preps += preps\n",
    "\n",
    "    num_docs = len(texts)\n",
    "    avg_sentences = total_sentences / num_docs if num_docs else 0\n",
    "    total_tokens, avg_tokens, top_10, down_10 = compute_token_stats(total_tokens_list, num_docs)\n",
    "    lemmas = get_lemmas(doc)\n",
    "\n",
    "    return {\n",
    "        \"num_sentences\": total_sentences,\n",
    "        \"avg_sentences_per_doc\": avg_sentences,\n",
    "        \"num_tokens\": total_tokens,\n",
    "        \"avg_tokens_per_doc\": avg_tokens,\n",
    "        \"top_10_tokens\": top_10,\n",
    "        \"down_10_tokens\": down_10,\n",
    "        \"num_nouns\": total_nouns,\n",
    "        \"num_verbs\": total_verbs,\n",
    "        \"num_prepositions\": total_preps,\n",
    "        \"lemmas\": lemmas\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentences: 199\n",
      "avg_sentences_per_doc: 199.0\n",
      "num_tokens: 5056\n",
      "avg_tokens_per_doc: 5056.0\n",
      "top_10_tokens: [('the', 224), ('of', 152), ('in', 125), ('to', 117), ('a', 113), ('and', 110), ('sage', 70), ('vocabulary', 70), ('for', 65), ('we', 60)]\n",
      "down_10_tokens: [('omer', 1), ('attendees', 1), ('iscol', 1), ('suggestions', 1), ('reviewers', 1), ('helpful', 1), ('kaj', 1), ('peter', 1), ('tamar', 1), ('operate', 1)]\n",
      "num_nouns: 1397\n",
      "num_verbs: 636\n",
      "num_prepositions: 700\n",
      "lemmas: ['proceeding', 'of', 'the', '17th', 'Conference', 'of', 'the', 'european', 'chapter', 'of', 'the', 'Association', 'for', 'Computational', 'Linguistics', ',', 'page', '623–635', 'May', '2', '-', '6', ',', '2023', '©', '2023', 'Association', 'for', 'Computational', 'Linguistics', 'incorporate', 'Context', 'into', 'Subword', 'Vocabularies', 'Shaked', 'Yehezkel', 'Blavatnik', 'School', 'of', 'Computer', 'Science', 'Tel', '-', 'Aviv', 'University', 'Tel', '-', 'Aviv', ',', 'Israel', 'shakedy@mail.tau.ac.il', 'Yuval', 'Pinter', 'Department', 'of', 'Computer', 'Science', 'Ben', '-', 'Gurion', 'University', 'of', 'the', 'Negev', 'Beer', 'Sheva', ',', 'Israel', 'uvp@cs.bgu.ac.il', 'abstract', 'most', 'current', 'popular', 'subword', 'tokenizer', 'be', 'train', 'base', 'on', 'word', 'frequency', 'statistic', 'over', 'a', 'corpus', ',', 'without', 'consider', 'informa-', 'tion', 'about', 'co', '-', 'occurrence', 'or', 'context', '.', 'Neverthe-', 'less', ',', 'the', 'result', 'vocabulary', 'be', 'use', 'in', 'lan-', 'guage', 'model', \"'\", 'highly', 'contextualized', 'setting', '.', 'we', 'present', 'SAGE', ',', 'a', 'tokenizer', 'that', 'tailor', 'sub-', 'word', 'for', 'their', 'downstream', 'use', 'by', 'bake', 'in', 'the', 'contextualize', 'signal', 'at', 'the', 'vocabulary', 'cre-', 'ation', 'phase', '.', 'we', 'show', 'that', 'SAGE', 'do', 'a', 'bet-', 'ter', 'job', 'than', 'current', 'widespread', 'tokenizer', 'in', 'keep', 'token', 'contexts', 'cohesive', ',', 'while', 'not', 'in-', 'curre', 'a', 'large', 'price', 'in', 'term', 'of', 'encode', 'effi-', 'ciency', 'or', 'domain', 'robustness', '.', 'SAGE', 'improve', 'performance', 'on', 'english', 'GLUE', 'classification', 'task', 'as', 'well', 'as', 'on', 'NER', ',', 'and', 'on', 'Inference', 'and', 'NER', 'in', 'Turkish', ',', 'demonstrate', 'its', 'robustness', 'to', 'language', 'property', 'such', 'as', 'morphological', 'exponence', 'and', 'agglutination', '.', '1', 'introduction', 'Much', 'of', 'the', 'research', 'space', 'in', 'current', 'NLP', 'be', 'focus', 'on', 'advance', 'model', ':', 'modify', 'pre-', 'training', 'objective', ',', 'improve', 'network', 'architec-', 'ture', ',', 'add', 'task', 'and', 'scheme', 'for', 'downstream', 'evaluation', '.', 'limited', 'work', 'be', 'dedicate', 'to', 'a', 'crucial', 'step', 'underlie', 'all', 'modern', 'large', 'language', 'model', '(', 'LLMs', ')', ',', 'namely', 'the', 'tokenization', 'phase', '.', 'in', 'order', 'to', 'process', 'a', 'give', 'string', 'of', 'text', ',', 'an', 'LLM', 'must', 'first', 'obtain', 'a', 'vector', 'representation', 'of', 'the', 'input', 'by', 'seg-', 'mente', 'it', 'into', 'token', '.', 'since', 'out', '-', 'of', '-', 'vocabulary', '(', 'OOV', ')', 'item', 'inhibit', 'the', 'performance', 'of', 'model', ',', 'current', 'tokenizer', 'produce', 'token', 'which', 'be', 'possi-', 'bly', 'proper', 'subsegment', 'of', 'input', 'word', ',', 'know', 'as', 'subword', '.', 'this', 'method', ',', 'popularize', 'by', 'system', 'such', 'as', 'WordPiece', '(', 'Schuster', 'and', 'Nakajima', ',', '2012', ')', ',', 'Byte', '-', 'Pair', 'Encoding', '(', 'BPE', ';', 'Sennrich', 'et', 'al', '.', ',', '2016', ')', 'and', 'UNIGRAMLM', '(', 'Kudo', ',', '2018', ')', ',', 'allow', 'any', 'word', 'to', 'be', 'represent', 'by', 'one', 'or', 'more', 'token', ',', 'remove', 'the', 'OOV', 'problem', 'while', 'allow', 'more', 'flexibility', 'in', 'determine', 'the', 'token', 'vocabulary', 'size', ',', 'which', 'ultimately', 'affect', 'model', 'speed', '(', 'mostly', 'through', 'BPE', 'his', 'son', 'Raj', 'ash', 'ri', 'Sud', 'h', 'ak', 'ar', 'have', 'p', 'enn', 'ed', 'dial', 'og', 'ue', 'and', 'song', 'for', 'some', 'film', 'that', 'be', 'dub', 'into', 'Telugu', '.', 'sage', 'his', 'son', 'Raj', 'ash', 'r', 'I', 'Sud', 'h', 'a', 'k', 'a', 'r', 'have', 'penn', 'e', 'd', 'dial', 'ogue', 'and', 'song', 'for', 'some', 'film', 'that', 'be', 'dub', 'into', 'Telugu', '.', 'BPE', 'this', 'gene', 'be', 'a', 'pseud', 'og', 'ene', 'in', 'human', 'and', 'most', 'other', 'prim', 'ate', '.', 'sage', 'this', 'gene', 'be', 'a', 'pseud', 'ogene', 'in', 'human', 'and', 'most', 'other', 'prim', 'ate', '.', 'BPE', 'the', 'St', 'o', 'og', 'es', 'work', 'for', 'Mir', 'acle', 'Det', 'ective', 'Agency', ',', 'sage', 'the', 'St', 'o', 'o', 'g', 'e', 's', 'work', 'for', 'Mir', 'acle', 'Det', 'ective', 'Agency', ',', 'Table', '1', ':', 'the', 'token', 'og', 'be', 'select', 'by', 'BPE', '(', 'vocabulary', 'of', 'size', '16,000', ')', 'for', 'achieve', 'the', 'frequency', 'objective', ',', 'but', 'be', 'discard', 'by', 'SAGE', 'for', 'fail', 'to', 'be', 'contextually', 'coherent', '.', 'these', 'example', 'from', 'the', 'corpus', 'demonstrate', 'some', 'different', 'context', '.', 'the', 'softmax', 'generation', 'target', ')', 'and', 'performance', '(', 'through', 'well', 'ability', 'to', 'represent', 'less', '-', 'frequent', 'word', ')', '.', 'one', 'potential', 'pitfall', 'of', 'both', 'BPE', 'and', 'uni-', 'GRAMLM', ',', 'as', 'well', 'as', 'their', 'propose', 'variant', '(', 'he', 'et', 'al', '.', ',', '2020', ';', 'Provilkov', 'et', 'al', '.', ',', '2020', ')', ',', 'be', 'that', 'they', 'be', 'train', 'on', 'word', 'frequency', 'statistic', 'alone', ',', 'without', 'consider', 'information', 'about', 'word', 'co', '-', 'occurrence', 'or', 'context', '.', 'at', 'the', 'same', 'time', ',', 'the', 'result', 'vocabu-', 'larie', 'be', 'use', 'in', 'highly', 'contextualize', 'setting', ',', 'the', 'llm', ',', 'where', 'a', 'single', 'subword', 'such', 'as', 'og', 'might', 'ap-', 'pear', 'in', 'very', 'different', 'context', 'derive', 'from', 'word', 'like', 'dial', 'og', 'ue', 'and', 'pseud', 'og', 'ene', '.', 'we', 'propose', 'a', 'sys-', 'tem', 'which', 'prepare', 'subword', 'for', 'their', 'downstream', 'use', 'by', 'bake', 'in', 'the', 'contextualize', 'signal', 'at', 'the', 'vocabulary', 'creation', 'step', '.', 'our', 'model', ',', 'SAGE', ',', 'use', 'the', 'skipgram', 'objective', '(', 'Mikolov', 'et', 'al', '.', ',', '2013', ')', 'over', 'a', 'corpus', 'as', 'the', 'basis', 'for', 'iteratively', 'eliminating', 'candidate', 'subword', 'from', 'an', 'initial', 'large', 'vocabulary', 'until', 'the', 'desire', 'vocabulary', 'size', 'have', 'be', 'reach', '.', 'as', 'table', '1', 'show', ',', 'SAGE', 'succeed', 'in', 'remove', 'the', 'ambiguous', 'og', 'token', ',', 'facilitate', 'distinct', 'contex-', 'tualization', 'procedure', 'for', 'the', 'example', 'sentence', '623', '(', 'take', 'from', 'Wikipedia', ')', '.', 'we', 'present', 'our', 'algorithm', ',', 'SAGE', ',', 'which', 'be', 'pred-', 'icate', 'on', 'iterative', 'pruning', 'of', 'contextually', 'noisy', 'token', 'from', 'the', 'vocabulary', ',', 'and', 'compare', 'its', 'effect', 'on', 'token', 'property', 'and', 'context', 'cohesion', 'with', 'BPE', 'both', 'in-', 'and', 'out', '-', 'of', '-', 'domain', ',', 'in', 'English', 'and', 'Turkish', '.', 'we', 'then', 'evaluate', 'its', 'performance', 'on', 'downstream', 'task', 'by', 'train', 'a', 'BERT', '-', 'base', 'LLM', '(', 'Devlin', 'et', 'al', '.', ',', '2019', ')', 'on', 'a', 'vocabulary', 'produce', 'by', 'both', 'tokeniz-', 'er', 'in', 'both', 'language', ',', 'demonstrate', 'substantial', 'improvement', 'on', 'most', 'english', 'GLUE', 'task', 'and', 'on', 'NER', ',', 'as', 'well', 'as', 'turkish', 'NLI', 'and', 'NER', '.', 'we', 'em-', 'phasize', 'that', 'as', 'oppose', 'to', 'most', 'current', 'tokenizer', 'variant', ',', 'our', 'model', 'be', 'a', '\"', 'plug', 'and', 'play', '\"', 'substitu-', 'tion', 'for', 'any', 'subword', 'token', 'vocabulary', ',', 'require', 'no', 'modification', 'in', 'the', 'inference', 'protocol', '(', 'or', 'code', ')', 'when', 'pre', '-', 'training', 'or', 'apply', 'an', 'applicable', 'llm', 'from', 'a', 'popular', 'share', 'library.1', '2', 'Subword', 'Vocabulary', 'Creation', 'the', 'method', 'use', 'to', 'tokenize', 'corpus', 'in', 'order', 'to', 'later', 'assign', 'token', 'with', 'continuous', 'vector', ',', 'or', 'em-', 'bedding', ',', 'have', 'evolve', 'over', 'the', 'year', '.', 'initially', ',', 'each', 'word', 'in', 'the', 'corpus', 'be', 'assign', 'its', 'own', 'em-', 'bed', '(', 'Collobert', 'and', 'Weston', ',', '2008', ';', 'mikolov', 'et', 'al', '.', ',', '2013', ')', '.', 'oov', ',', 'i.e.', 'word', 'not', 'appear', 'in', 'the', 'original', 'training', 'corpus', 'or', 'below', 'a', 'certain', 'fre-', 'quency', 'threshold', ',', 'would', 'receive', 'a', 'special', '(', 'but', 'iden-', 'tical', ')', '\"', 'UNK', '\"', 'vector', '.', 'Subword', 'tokenizer', '(', 'Schuster', 'and', 'Nakajima', ',', '2012', ';', 'Wu', 'et', 'al', '.', ',', '2016', ')', 'be', 'intro-', 'duce', 'to', 'alleviate', 'this', 'issue', ',', 'allow', 'segmentation', 'of', 'all', 'text', 'into', 'embeddable', 'unit', '(', 'assume', 'no', 'un-', 'see', 'character', ',', 'a', 'much', 'more', 'relaxed', 'constraint', 'for', 'language', 'use', 'alphabetical', 'script', ')', '.', 'the', 'training', 'process', 'use', 'to', 'create', 'a', 'subword', 'vocabulary', 'from', 'which', 'the', 'model', 'then', 'decode', 'text', 'input', 'involves', 'optimize', 'an', 'encoding', 'objective', 'over', 'a', 'large', 'cor-', 'pus', '.', 'to', 'date', ',', 'all', 'tokenizer', 'use', 'in', 'practice', 'in', 'large', 'model', 'focus', 'on', 'efficiency', 'and', 'information-', 'theoretic', 'objective', ',', 'and', 'reduce', 'the', 'corpus', 'to', 'a', 'unigram', 'frequency', 'count', 'of', 'space', '-', 'delimit', 'word', ',', 'reduce', 'calculation', 'time', 'but', 'lose', 'all', 'contextual', 'signal', '.', 'sage', 'reintroduce', 'the', 'contextual', 'depen-', 'dencie', 'between', 'word', 'into', 'vocabulary', 'creation', 'via', 'a', 'two', '-', 'stage', 'process', ',', 'namely', 'over', '-', 'application', 'of', 'BPE', 'follow', 'by', 'iterative', 'pruning', 'use', 'idea', 'inspire', 'by', 'UNIGRAMLM', 'and', 'SKIPGRAM', '.', 'we', 'briefly', 'present', 'these', 'algorithm', 'before', 'tie', 'they', 'together', 'into', 'SAGE', '.', '1our', 'code', 'and', 'model', 'be', 'available', 'at', 'www.github', '.', 'com', '/', 'MeLeLbgu', '/', 'SaGe', '.', 'Algorithm', '1', 'Byte', '-', 'pair', 'encode', 'vocabulary', 'cre-', 'ation', '(', 'Gage', ',', '1994', ';', 'Sennrich', 'et', 'al', '.', ',', '2016', ')', 'Input', ':', 'Corpus', 'C', ',', 'Vocabulary', 'final', 'size', 'v', '.', 'output', ':', 'Vocabulary', 'v', 'of', 'size', 'v', '(', 'order', ')', '.', '1', ':', 'procedure', 'bpe(c', ',', 'V', ')', '2', ':', 'v', '←all', 'unique', 'character', 'in', 'C', '3', ':', 'while', '|v|', '<', 'v', 'do', '▷merge', 'token', '4', ':', '⟨tl', ',', 'tr⟩←most', 'frequent', 'bigram', 'in', 'C', '5', ':', 'tNEW', '←tL', '⊕tR', '▷make', 'new', 'token', '6', ':', 'V', '←V', '⊕[tNEW', ']', '7', ':', 'C.ReplaceAll(⟨tL', ',', 'tR⟩', ',', 'tNEW', ')', '8', ':', 'end', 'while', '9', ':', 'return', 'V', '10', ':', 'end', 'procedure', 'Byte', '-', 'Pair', 'Encoding', '.', 'the', 'BPE', 'algorithm', 'cre-', 'ate', 'a', 'vocabulary', '\"', 'bottom', '-', 'up', '\"', ',', 'start', 'with', 'all', 'sin-', 'gle', 'character', 'from', 'the', 'alphabet', ',', 'iteratively', 'add', 'token', 'until', 'reach', 'the', 'desire', 'vocabulary', 'size', '.', 'in', 'each', 'iteration', ',', 'the', 'add', 'token', 'be', 'the', 'concate-', 'nation', 'of', 'the', 'most', 'frequent', 'adjacent', 'pair', 'of', 'ex-', 'isting', 'token', '(', 'see', 'Algorithm', '1', ')', '.', 'the', 'default', 'set-', 'ting', 'of', 'the', 'algorithm', '’s', 'most', 'popular', 'implementa-', 'tion', '(', 'Kudo', 'and', 'Richardson', ',', '2018', ')', 'restrict', 'token', 'addition', 'within', 'word', 'boundary', ',', 'facilitate', 'train-', 'ing', 'from', 'unigram', 'frequency', '.', 'in', 'addition', ',', 'LLM', 'tokenizer', 'use', 'BPE', '(', 'Liu', 'et', 'al', '.', ',', '2019', ';', 'Radford', 'et', 'al', '.', ',', '2019', ';', 'Wolf', 'et', 'al', '.', ',', '2020', ')', 'decode', 'sequence', 'not', 'by', 'apply', 'merge', 'by', 'order', 'of', 'the', 'vocabulary', ',', 'as', 'originally', 'dictate', 'by', 'the', 'algorithm', ',', 'but', 'through', 'greedy', 'large', '-', 'subsequence', 'leave', '-', 'to', '-', 'right', 'inference', '.', 'Unigram', 'Language', 'Model', '.', 'UNIGRAMLM', 'of-', 'fer', 'a', 'top', '-', 'down', 'vocabulary', 'creation', 'process', ',', 'start-', 'ing', 'with', 'an', 'initial', 'vocabulary', 'of', 'all', 'substring', 'in', 'the', 'input', 'corpus', 'and', 'pruning', 'token', 'iteratively', 'until', 'reach', 'the', 'desire', 'vocabulary', 'size', '.', 'the', 'pruning', 'procedure', 'involve', 'calculate', 'the', 'overall', 'unigram', 'likelihood', 'of', 'the', 'corpus', 'with', 'the', 'current', 'vocabulary', 'versus', 'a', 'vocabulary', 'lack', 'the', 'candidate', 'pruning', 'token', '(', 'see', 'Algorithm', '2', 'for', 'detail', ')', ',', 'which', 'we', 'refer', 'to', 'as', 'the', 'ablation', 'objective', '.', 'under', 'this', 'system', ',', 'decode', 'be', 'ideally', 'perform', 'by', 'consider', 'prob-', 'ability', 'of', 'all', 'possible', 'segmentation', 'use', ',', 'e.g.', ',', 'the', 'Viterbi', 'algorithm', ';', 'again', ',', 'common', 'practice', 'be', 'to', 'use', 'leave', '-', 'to', '-', 'right', 'greedy', 'decoding', '.', 'Skipgram', 'Objective', '.', 'the', 'skipgram', 'objec-', 'tive', '(', 'Mikolov', 'et', 'al', '.', ',', '2013', ')', 'formalize', 'the', 'relation', 'between', 'a', 'target', 'token', 't', 'and', 'its', 'context', ',', 'ask', 'whether', 'context', 'token', 'c', 'within', 'a', 'window', 'Wt', 'of', 'pre', '-', 'define', 'size', 'can', 'be', 'predict', 'from', 't.', 'these', 'prediction', 'be', 'do', 'via', 'sigmoid', 'activation', 'over', 'the', 'inner', 'product', 'of', 'embedding', 'train', 'for', 'target', '(', 'e(t', ')', ')', 'and', 'contexts', '(', 'e(c', ')', ')', '.', 'when', 'aggregate', 'over', '624', 'Algorithm', '2', 'UNIGRAMLM', 'vocabulary', 'creation', '(', 'Kudo', ',', '2018', ')', '.', 'n', 'arg', 'minx', 'denote', 'the', 'n', 'bottom-', 'rank', 'element', 'in', 'X.', 'input', ':', 'Corpus', 'C', ',', 'Vocabulary', 'final', 'size', 'v', ',', 'pruning', 'batch', 'size', 'k.', 'output', ':', 'Vocabulary', 'v', 'of', 'size', 'v', '.', '1', ':', 'procedure', 'unigramlm(c', ',', 'v', ')', '2', ':', 'v', '←all', 'substring', 'occur', 'more', 'than', 'once', 'in', 'c', '3', ':', 'while', '|v|', '>', 'v', 'do', '▷Prune', 'tokens', '4', ':', 'x(j', ')', '←tokenize(C', ',', 'V', ')', '5', ':', 'l(v', ')', '←', '|C|', 'X', 'j=1', 'log', '\\x10', 'P(X(j', ')', ')', '\\x11', '6', ':', 'for', 'all', 't', '∈v', 'do', ':', '▷calculate', 'ablation', 'objective', '7', ':', 'losst', '←L(V', '\\\\', '{', 't', '}', ')', '−L(V', ')', '8', ':', 'end', 'for', '9', ':', 'P', '←min(k', ',', '|V|', '−V', ')', 'arg', 'mint∈V(losst', ')', '10', ':', 'V', '←V', '\\\\', 'P', '▷Prune', '11', ':', 'end', 'while', '12', ':', 'return', 'V', '13', ':', 'end', 'procedure', 'all', 'token', 'in', 'a', 'corpus', ',', 'SKIPGRAM', 'can', 'be', 'use', 'as', 'a', 'total', 'likelihood', 'measure', ',', 'approximate', 'its', 'overall', 'contextual', 'cohesion', ':', 'L(V', ',', 'C', ')', '=', '−', 'X', 't∈tok(C', ',', 'V', ')', 'x', 'cj∈wt', 'log', '\\x10', 'σ(E(T', ')', 't', '·', 'e(c', ')', 'cj', ')', '\\x11', '.', '(', '1', ')', 'as', 'token', 'vocabulary', 'or', 'their', 'inference', 'method', 'change', ',', 'so', 'do', 'the', 'target', 'sequence', 'and', 'their', 'con-', 'text', ',', 'result', 'in', 'difference', 'in', 'aggregate', 'likeli-', 'hood', 'which', 'can', 'then', 'act', 'as', 'score', 'compare', 'one', 'tokenization', 'to', 'another', '.', 'we', 'use', 'this', 'behavior', 'as', 'the', 'ablation', 'objective', 'for', 'SAGE', '.', '3', 'SAGE', 'Vocabulary', 'Creation', 'SAGE2', 'be', 'a', 'top', '-', 'down', 'tokenizer', ',', 'follow', 'UN-', 'IGRAMLM', '’s', 'general', 'procedure', ',', 'incorporate', 'a', 'skipgram', 'objective', 'as', 'its', 'vocabulary', 'trimming', 'rule', '.', 'give', 'an', 'initial', 'vocabulary', 'v', 'and', 'a', 'corpus', 'C', ',', 'SAGE', 'compute', 'a', 'skipgram', 'embed', 'space', 'over', 'v', 'which', 'provide', 'it', 'with', 'an', 'overall', 'likelihood', 'over', 'C', 'as', 'in', '(', '1', ')', '.', 'it', 'then', 'proceed', 'to', 'calculate', 'the', 'loss', 'of', 'each', 'token', 'in', 'the', 'vocabulary', 'be', 'it', 'to', 'be', 'remove', ',', 'eliminate', 'the', 'token', 'incur', 'minimal', 'loss', 'and', 're', '-', 'tokenize', 'the', 'corpus', 'accord', 'to', 'the', 'update', 'vocabulary', ',', 'repeat', 'this', 'procedure', 'until', 'reach', 'the', 'desire', 'vocabulary', 'size', 'v', '.', 'having', 'learn', 'this', 'vocabulary', ',', 'downstream', 'inference', 'pro-', 'ceed', 'exactly', 'as', 'in', 'the', 'other', 'segmentation', '-', 'base', 'method', ',', 'in', 'a', 'greedy', 'leave', '-', 'to', '-', 'right', 'manner', '.', 'SAGE', 'can', 'also', 'be', 'adapt', 'to', 'anticipate', 'other', 'decode', '2the', 'name', 'be', 'not', 'an', 'acronym', ';', 'it', 'be', 'intend', 'to', 'evoke', 'SkipGram', 'while', 'maintain', 'the', '\"', 'suffix', '\"', 'of', 'BPE', '.', 'Algorithm', '3', 'sage', 'vocabulary', 'creation', '.', 'n', 'arg', 'minx', 'denote', 'the', 'n', 'bottom', '-', 'rank', 'element', 'in', 'X.', 'input', ':', 'Corpus', 'C', ',', 'Vocabulary', 'final', 'size', 'v', ',', 'basic', 'tok-', 'enizer', 't', ',', 'overshoot', 'factor', 'n', ',', 'pruning', 'batch', 'size', 'k', ',', 'likelihood', 'recalculation', 'frequency', 'm', ',', 'size', 'of', 'pruning', 'candidate', 'set', 'M', ',', 'embed', 'recalculation', 'frequency', 'l.', 'output', ':', 'Vocabulary', 'v', 'of', 'size', 'v', '.', '1', ':', 'procedure', 'sage(c', ',', 'v', ')', '2', ':', 'V', '←T', '(', 'C', ',', 'n', '·', 'v', ')', '3', ':', 'I', '←0', '4', ':', 'while', '|v|', '>', 'v', 'do', '5', ':', 'if', 'I', '≡0', '(', 'mod', 'l', '×', 'm', ')', 'then', '6', ':', 'EV', '←Word2Vec(V', ')', '▷embedde', 'table', '7', ':', 'end', 'if', '8', ':', 'L(V', ',', 'C', ')', '←SGObj(EV', ',', 'C', ')', '▷total', 'likelihood', '(', '1', ')', '9', ':', 'if', 'I', '≡0', '(', 'mod', 'm', ')', 'then', '▷update', 'bottom', 'set', '10', ':', 'for', 'all', 't', '∈v', 'do', ':', '11', ':', 'losst', '←L(V', '\\\\', '{', 't', '}', ',', 'C', ')', '−L(V', ',', 'C', ')', '12', ':', 'end', 'for', '13', ':', 'Vbot', '←m', 'arg', 'mint∈V(losst', ')', '14', ':', 'else', '▷update', 'loss', 'for', 'bottom', 'set', '15', ':', 'for', 'all', 't', '∈vbot', 'do', ':', '16', ':', 'losst', '←L(V', '\\\\', '{', 't', '}', ',', 'C', ')', '−L(V', ',', 'C', ')', '17', ':', 'end', 'for', '18', ':', 'end', 'if', '19', ':', 'P', '←min(k', ',', '|V|', '−V', ')', 'arg', 'mint∈Vbot(losst', ')', '20', ':', 'Vbot', '←Vbot', '\\\\', 'P', '▷Prune', '21', ':', 'V', '←V', '\\\\', 'P', '22', ':', 'I', '←i', '+', '1', '23', ':', 'end', 'while', '24', ':', 'return', 'V', '25', ':', 'end', 'procedure', 'algorithm', ',', 'by', 'change', 'the', 're', '-', 'tokenization', 'step', 'accordingly', '.', 'in', 'practice', ',', 'apply', 'the', 'full', 'process', 'describe', 'above', 'introduce', 'multiple', 'source', 'of', 'considerable', 'computational', 'complexity', ':', 'for', 'example', ',', 'calculat-', 'e', 'the', 'ablation', 'objective', 'for', 'each', 'token', 'in', 'each', 'it-', 'eration', 'produce', 'a', 'quadratic', 'amount', 'of', 'calculation', 'over', 'the', 'entire', 'corpus', ';', 'recalculate', 'embedding', 'for', 'an', 'update', 'vocabulary', 'be', 'similarly', 'unreason-', 'able', 'to', 'perform', 'at', 'each', 'iteration', '.', 'we', 'ameliorate', 'these', 'and', 'other', 'source', 'of', 'complexity', 'use', 'a', 'se-', 'rie', 'of', 'heuristic', 'find', 'in', 'preliminary', 'experiment', 'to', 'be', 'minimally', 'disruptive', 'to', 'precision', 'of', 'likeli-', 'hood', 'calculation', '.', 'we', 'will', 'now', 'describe', 'these', 'heuristic', ',', 'all', 'depict', 'in', 'Algorithm', '3', '.', 'first', ',', 'in-', 'stead', 'of', 'initialize', 'the', 'vocabulary', 'as', 'the', 'full', 'set', 'of', 'possible', 'character', 'sequence', 'in', 'the', 'corpus', ',', 'as', 'in', 'UNIGRAMLM', ',', 'we', 'use', 'any', 'exist', 'noncontextual', 'tokenizer', 'such', 'as', 'BPE', 'to', 'learn', 'a', 'vocabulary', 'large', 'than', 'V', 'by', 'a', 'factor', 'of', 'n', ',', 'and', 'begin', 'the', 'prune', 'pro-', 'cess', 'from', 'there', '.', 'next', ',', 'instead', 'of', 'remove', 'a', 'single', 'token', 'from', 'the', 'bottom', 'of', 'the', 'loss', '-', 'rank', 'vocab-', 'ulary', ',', 'we', 'remove', 'a', 'batch', 'of', 'the', 'k', 'bottom', 'token', '625', 'Sentence', 'fragment', '.', '.', '.', 'use', 'of', 'an', 'include', 'directive', 'be', 'when', 'refer', 'to', '.', '.', '.', 'Tokenization', 'use', 'v', 'use', 'of', 'an', 'includ', '[', 'e', 'direct', 'I', 've', 'be', 'when', ']', 'ref', 'er', 'r', 'ing', 'to', 'Tokenization', 'use', 'V', '\\\\', '{', 'includ', '}', 'use', 'of', 'an', 'inc', 'l', 'u', '[', 'de', 'direct', 'I', 've', 'be', 'when', ']', 'ref', 'er', 'r', 'ing', 'to', 'Table', '2', ':', 'the', 'effect', 'of', 'retokenization', 'on', 'a', 'context', 'window', 'of', 'width', '2', '(', 'in', 'bracket', ')', 'surround', 'a', 'target', 'token', '(', 'in', 'bold', ')', '.', 'a', 'left', '-', 'side', 'context', 'token', 'have', 'be', 'replace', 'as', 'a', 'result', 'of', 'an', 'out', '-', 'of', '-', 'window', 'vocabulary', 'ablation', '.', 'each', 'time', ',', 'as', 'do', 'unigramlm.3', 'to', 'avoid', 'fre-', 'quent', 'loss', 'recalculation', ',', 'we', 'recompute', 'the', 'entire', 'likelihood', 'set', 'once', 'every', 'm', 'ablation', 'step', ',', 'and', 'only', 'keep', 'the', 'bottom', 'M', 'tokens', 'as', 'prune', 'candidate', 'for', 'the', 'next', 'm', 'step', '.', 'our', 'preliminary', 'experiment', 'support', 'this', 'decision', ',', 'as', 'we', 'find', 'the', 'ranked', 'list', 'of', 'loss', 'tend', 'to', 'stay', 'relatively', 'stable', 'over', 'dozen', 'of', 'batch', '-', 'prune', 'iteration', '.', 'lastly', ',', 'to', 'avoid', 'the', 'costly', 're', '-', 'training', 'of', 'the', 'embed', 'matrix', 'for', 'all', 'token', 'give', 'the', 'update', 'corpus', ',', 'which', 'only', 'result', 'in', 'minor', 'change', 'in', 'likelihood', 'during', 'subsequent', 'iter-', 'ation', ',', 'we', 'only', 'perform', 'it', 'every', 'l', 'iteration', 'batch', ',', 'i.e.', 'after', 'the', 'ablation', 'of', 'k', '×', 'm', '×', 'l', 'tokens', '.', 'n', ',', 'k', ',', 'l', ',', 'm', 'and', 'M', 'be', 'all', 'algorithm', 'hyperparameter', 'tune', 'empirically', 'base', 'on', 'desire', 'runtime', ',', 'corpus', 'size', 'and', 'vocabulary', 'size', '.', 'Contextual', 'Loss', '.', 'in', 'order', 'to', 'calculate', 'the', 'per-', 'token', 'skipgram', 'likelihood', 'loss', ',', 'all', 'sentence', 'where', 'a', 'token', 't', 'occur', 'need', 'to', 'be', 're', '-', 'segment', 'accord', 'to', 'V', '\\\\', '{', 't', '}', ',', 'and', 'their', 'new', 'likelihood', 'record', '.', 'to', 'support', 'perform', 'this', 'calculation', 'on', 'a', 'large', 'scale', ',', 'we', 'maintain', 'a', 'mapping', 'of', 'token', 'to', 'sentence', 'contain', 'they', ',', 'as', 'well', 'as', 'these', 'sen-', 'tence', \"'\", 'current', 'likelihood', '.', 'this', 'must', 'be', 'do', 'at', 'the', 'sentence', 'level', 'rather', 'than', 'the', 'window', 'level', ',', 'since', 'a', 'remain', 'suffix', 'from', 'an', 'out', '-', 'of', '-', 'window', 're', '-', 'tokenization', 'may', 'combine', 'with', 'in', '-', 'window', 'char-', 'acter', 'and', 'form', 'different', 'token', 'sequence', 'replace-', 'ment', 'at', 'a', 'give', 'stage', '.', 'consider', 'the', 'example', 'in', 'Table', '2', ',', 'where', 're', '-', 'tokenization', 'result', 'in', 'the', 're-', 'placement', 'of', 'a', 'context', 'token', 'for', 'a', 'distant', 'target', '.', 'Negative', 'Sampling', '.', 'the', 'original', 'SKIPGRAM', 'objective', 'use', 'negative', 'sample', 'to', 'estimate', 'con-', 'text', 'probability', '.', 'since', 'our', 'application', 'of', 'SKIP-', 'GRAM', 'within', 'the', 'vocabulary', 'creation', 'algorithm', '(', 'independent', 'of', 'the', 'embedding', 'train', 'proce-', 'dure', ')', 'include', 'only', 'likelihood', 'estimation', 'with', 'no', 'parameter', 'update', ',', 'we', 'do', 'not', 'sample', 'negative', 'to-', 'ken', ',', 'a', 'process', 'which', 'would', 'introduce', 'substantial', 'noise', 'and', 'complexity', '.', '3as', 'in', 'UNIGRAMLM', 'and', 'other', 'ablation', '-', 'base', 'vocabu-', 'larie', ',', 'single', '-', 'character', 'token', 'be', 'never', 'remove', 'from', 'the', 'vocabulary', ',', 'in', 'order', 'to', 'allow', 'for', 'all', 'in', '-', 'alphabet', 'word', 'to', 'be', 'tokenize', '.', '4', 'SAGE', 'Vocabulary', 'Properties', 'for', 'an', 'analysis', 'of', 'our', 'modify', 'algorithm', '’s', 'advan-', 'tage', ',', 'we', 'train', 'vocabulary', 'of', 'a', 'pre', '-', 'determine', 'size', 'use', 'both', 'BPE', 'and', 'SAGE', '.', 'we', 'select', '|v|', '=', '16', ',', '000', ',', 'and', 'obtain', 'corpora', 'for', 'English', '(', '750,000', 'line', 'from', 'the', 'August', '2022', 'Wikipedia', 'dump', ')', 'and', 'turkish', '(', 'the', 'entire', 'text', 'of', 'the', 'September', '2022', 'Wikipedia', 'dump', ')', ',', 'opt', 'for', 'language', 'that', 'share', 'the', 'Latin', 'alphabet', 'but', 'differ', 'in', 'family', '(', 'Indo-', 'European', 'vs.', 'Turkic', ')', 'and', ',', 'crucially', ',', 'in', 'morpholog-', 'ical', 'property', ':', 'English', 'be', 'a', 'low', '-', 'exponence', ',', 'low-', 'synthesis', 'language', ',', 'while', 'turkish', 'feature', 'multiple', 'inflectional', 'exponence', 'and', 'high', 'verbal', 'synthesis', ',', 'as', 'well', 'as', 'highly', 'agglutinative', 'morphology', '(', 'Bickel', 'and', 'Nichols', ',', '2013a', ',', 'b', ')', '.', 'we', 'use', 'the', 'follow', 'hy-', 'perparameter', 'setting', 'to', 'compute', 'the', 'vocabulary', ':', 'initial', 'vocab', 'size', '20', ',', '000', '(', 'or', 'n', '=', '1.25', ')', ',', 'l', '=', '4', ',', 'k', '=', '100', ',', 'M', '=', '1500', ',', 'm', '=', '10', '.', 'we', 'use', 'the', 'Gen-', 'sim', 'package', 'to', 'train', 'the', 'skipgram', 'model', '(', 're-', 'hurek', 'and', 'Sojka', ',', '2011', ')', ',', 'and', 'Sentencepiece', '(', 'Kudo', 'and', 'Richardson', ',', '2018', ')', 'to', 'obtain', 'the', 'initial', 'BPE', 'vocabularies.4', 'More', 'hyperparameter', 'be', 'detail', 'in', 'Appendix', 'A.', 'we', 'present', 'an', 'analysis', 'of', 'the', 'result', 'vocabu-', 'larie', ',', 'highlight', 'the', 'advantage', 'and', 'trade', '-', 'off', 'exhibit', 'by', 'context', '-', 'base', 'subword', 'tokenization', '.', 'generally', 'speak', ',', 'most', 'of', 'the', 'token', 'discard', 'from', 'SAGE', '’s', 'initial', 'vocabulary', 'appear', 'in', 'the', 'base-', 'line', 'BPE', '’s', 'final', 'vocabulary', '.', 'among', 'the', 'differ-', 'ence', 'between', 'the', 'vocabulary', 'be', 'many', 'short', 'to-', 'ken', 'that', 'appear', 'in', 'BPE', '’s', 'but', 'not', 'SAGE', '’s', ',', 'proper', 'substring', 'of', 'long', 'token', 'also', 'appear', 'in', 'the', 'BPE', 'vocabulary', '.', 'this', 'be', 'due', 'to', 'BPE', '’s', 'bottom', '-', 'up', 'merge', 'table', 'construction', ',', 'which', 'force', 'retention', 'of', 'the', 'entire', 'chain', 'of', 'tokens', 'create', ':', 'if', 'the', 'be', 'part', 'of', 'the', 'vocabulary', ',', 'either', 'th', 'or', 'he', 'must', 'also', 'be', 'there', '.', 'while', 'essential', 'for', 'the', 'original', 'intend', 'decode', 'process', ',', 'actual', 'implementation', 'of', 'greedy', 'decod-', 'ing', 'have', 'no', 'need', 'for', 'this', 'property', '.', 'SAGE', '’s', 'ini-', 'tial', 'vocabulary', 'share', 'this', 'characteristic', ',', 'but', 'the', 'trim', 'process', 'allow', 'any', 'token', 'to', 'be', 'ablate', ',', '4since', 'BPE', 'augment', 'its', 'vocabulary', 'iteratively', ',', 'the', 'base-', 'line', 'BPE', 'vocabulary', 'be', 'a', 'proper', 'subset', 'of', 'that', 'use', 'to', 'initialize', 'SAGE', '.', '626', 'figure', '1', ':', 'token', 'length', 'distribution', 'of', 'BPE', '’s', 'vocabulary', 'vs.', 'SAGE', '’s', 'on', 'English', '.', 'more', 'frequent', 'in', 'SAGE', 'BPE', 'e', 's', 'es', 'ic', 'ing', 'ist', 'ing', 'ff', 'ation', 'eat', 'ation', 'ate', 'Table', '3', ':', 'Tokens', 'with', 'high', 'difference', 'in', 'frequency', 'be-', 'tween', 'tokenization', '(', 'english', 'model', ')', '.', 'include', 'those', 'in', 'the', 'middle', 'of', 'merge', 'chain', '.', 'an-', 'other', 'difference', 'find', 'between', 'the', 'vocabulary', 'be', 'the', 'strong', 'preference', 'of', 'SAGE', 'for', 'word', '-', 'initial', 'token', '.', '83', '%', 'of', 'the', 'token', 'that', 'appear', 'in', 'SAGE', '’s', 'vocabulary', 'but', 'not', 'in', 'BPE', '’s', 'be', 'word', '-', 'initial', ',', 'com-', 'pare', 'to', 'only', '22', '%', 'of', 'the', 'BPE', '-', 'only', 'token', '.', 'this', 'be', 'reasonable', ',', 'since', 'a', 'token', 'survive', 'SAGE', '’s', 'abla-', 'tion', 'step', 'exhibit', 'high', 'loss', 'for', 'the', 'condition', 'of', 'its', 'removal', ',', 'which', 'be', 'arguably', 'the', 'case', 'when', 'a', 'nearby', 'target', 'word', 'need', 'to', 'predict', 'a', 'word', '-', 'initial', 'context', '.', 'Token', 'Length', '.', 'figure', '1', 'show', 'a', 'histogram', 'of', 'token', 'length', '(', 'in', 'character', ')', 'for', 'the', '16,000', '-', 'token', 'SAGE', 'and', 'BPE', 'vocabulary', 'in', 'English', '(', 'result', 'on', 'turkish', 'be', 'similar', ')', '.', 'SAGE', 'clearly', 'select', 'long', 'token', 'for', 'its', 'vocabulary', ',', 'again', 'a', 'sensible', 'outcome', 'give', 'their', 'high', 'chance', 'of', 'be', 'contextually', 'coherent', '.', 'the', 'difference', 'be', 'most', 'stark', 'with', 'token', 'of', 'length', '2', 'and', '3', ';', 'when', 'consider', 'only', 'token', 'appear', 'in', 'exactly', 'one', 'of', 'the', 'final', 'vocabulary', ',', 'we', 'find', 'that', '56', '%', 'of', 'BPE', '-', 'only', 'token', 'be', 'of', 'length', '2', 'and', '3', ',', 'while', '55', '%', 'of', 'SAGE', '-', 'only', 'token', 'be', 'of', 'length', '5', 'and', 'above', '.', 'Token', 'Frequency', '.', 'we', 'compute', 'the', 'frequency', 'of', 'token', 'in', 'the', 'encoding', 'form', 'of', 'the', 'English', 'train-', 'ing', 'corpus', ',', 'once', 'use', 'SAGE', 'vocabulary', 'and', 'once', 'figure', '2', ':', 'number', 'of', 'subword', 'require', 'to', 'tokenize', 'a', 'word', ',', 'collect', 'over', 'the', 'original', 'english', 'training', 'cor-', 'pus', '.', 'use', 'BPE', '’s', '.', 'in', 'table', '3', 'we', 'show', 'some', 'of', 'the', 'token', 'with', 'the', 'big', 'difference', 'in', 'frequency', 'be-', 'tween', 'SAGE', 'and', 'BPE', 'tokenization', '.', 'we', 'can', 'see', 'SAGE', 'revert', 'to', 'single', '-', 'character', 'token', 'consider-', 'ably', 'more', 'often', 'than', 'BPE', '(', 'also', 'demonstrate', 'in', 'the', 'last', 'example', 'in', 'Table', '1', ')', '.', 'we', 'view', 'this', 'as', 'a', 'fea-', 'ture', 'of', 'context', '-', 'base', 'tokenization', '—', 'its', 'vocabulary', 'be', 'partition', 'between', '(', 'mostly', 'short', ')', 'token', 'that', 'be', 'highly', 'ambiguous', 'in', 'context', 'and', '(', 'mostly', 'long', ')', 'to-', 'ken', 'that', 'have', 'coherent', 'context', '.', 'at', 'the', 'same', 'time', ',', 'BPE', 'be', 'rife', 'with', 'token', 'that', 'be', 'medially', 'ambigu-', 'ous', 'contextually', ',', 'whose', 'result', 'embedding', 'can', 'be', 'neither', 'useful', 'nor', 'completely', 'ignorable', ',', 'add', 'noise', 'to', 'the', 'representation', 'sequence', '.', 'as', 'a', 'result', ',', 'sage', 'break', 'down', 'complex', 'suffix', ',', 'which', 'in', 'English', 'be', 'compositional', ',', 'into', 'their', 'constituent', 'morphology', '.', 'the', 'suffix', 'ing', 'be', 'thus', 'dismantle', 'to', 'ing', 's', ',', 'whereas', 'BPE', 'reserve', 'a', 'token', 'for', 'it', ',', 'mostly', 'unhelpful', 'in', 'itself', '.', 'Subword', 'Fertility', '.', 'Fertility', ',', 'as', 'define', 'in', 'the', 'statistical', 'machine', 'translation', 'literature', ',', 'refer', 'to', 'the', 'average', 'number', 'of', 'subword', 'produce', 'per', 'to-', 'kenize', 'word', '.', 'figure', '2', 'exhibit', 'a', 'histogram', 'of', 'all', 'English', 'corpus', 'word', 'by', 'their', 'subword', 'length', ',', 'use', 'the', 'BPE', 'vocabulary', 'and', 'the', 'SAGE', 'vocabu-', 'lary', '.', 'although', 'SAGE', 'retain', 'more', 'word', 'as', 'single', 'token', ',', 'it', 'trade', 'they', 'off', 'with', 'more', 'word', 'hav-', 'e', 'five', 'subword', 'or', 'more', ',', 'compare', 'with', 'BPE', '’s', 'abundance', 'of', 'word', 'with', '2', 'and', '3', 'subword', '.', 'this', 'follow', 'the', 'trend', 'describe', 'so', 'far', ',', 'of', 'SAGE', '’s', 'pref-', 'erence', 'for', 'dismantle', 'unknown', 'word', 'into', 'mean-', 'ingless', 'single', '-', 'character', 'token', 'rather', 'than', 'confus-', 'ing', ',', 'ambiguous', 'length-2', 'and', 'length-3', 'token', '.', 'we', 'believe', 'that', 'BPE', '’s', 'behavior', 'harm', 'text', 'understand-', '627', 'figure', '3', ':', 'number', 'of', 'distinct', 'neighbor', 'each', 'token', 'encounter', 'in', 'a', 'width-5', 'window', ',', 'top', '200', ',', 'Turkish', '.', 'ing', 'in', 'suggest', 'that', 'these', 'ambiguous', 'fragment', '(', 'consider', '\"', 'og', '\"', ')', 'have', 'some', 'meaning', 'that', 'an', 'LLM', 'can', 'try', 'and', 'learn', ',', 'whereas', 'SAGE', '’s', 'single', '-', 'character', 'breakdown', 'indicate', 'a', 'word', 'that', '’', 'truly', 'unknown', 'and', 'can', 'not', 'be', 'infer', 'by', 'compose', 'constituent', 'in', '-', 'vocab', 'subword', '.', 'fertility', 'translate', 'to', 'a', 'trade', '-', 'off', 'in', 'encode', 'efficiency', 'to', 'SAGE', '’s', 'contextual', 'advantage', ':', 'a', 'sam-', 'ple', 'of', '150', 'K', 'line', 'from', 'English', 'Wikipedia', 'be', 'en-', 'code', 'by', '4', 'million', 'BPE', 'token', ',', 'optimize', 'only', 'an', 'information', '-', 'theoretic', 'objective', ',', 'whereas', 'SAGE', 'produce', '4.5', 'million', '.', 'having', 'say', 'that', ',', 'this', 'inef-', 'ficiency', 'might', 'be', 'far', 'offset', 'during', 'LLM', 'pre-', 'training', ':', 'we', 'propose', 'that', 'contextually', 'coherent', 'token', 'will', 'require', 'few', 'update', 'step', 'in', 'order', 'to', 'achieve', 'useful', 'embed', 'parameter', ',', 'help', 'the', 'model', 'converge', 'fast', 'compare', 'to', 'BPE', 'token', '.', 'we', 'leave', 'test', 'this', 'hypothesis', 'to', 'future', 'work', '.', 'Contextual', 'Exponence', '.', 'to', 'determine', 'the', 'degree', 'to', 'which', 'SAGE', 'effectively', 'optimize', 'token', \"'\", 'con-', 'textual', 'soundness', ',', 'which', 'be', 'its', 'ultimate', 'goal', ',', 'we', 'plot', 'the', 'number', 'of', 'distinct', 'neighbor', 'each', 'token', 'encounter', 'throughout', 'the', 'training', 'corpus', ',', 'rank', 'from', 'high', 'to', 'low', ',', 'in', 'Figure', '3', '.', 'the', 'very', 'top', 'of', 'the', 'ranking', 'be', 'occupy', 'by', 'single', '-', 'character', 'token', 'which', 'be', 'context', '-', 'null', 'by', 'design', ',', 'which', 'SAGE', 'make', 'the', 'most', 'of', 'by', 'place', 'in', 'almost', 'all', 'context', '.', 'after', 'a', 'few', 'dozen', 'token', ',', 'SAGE', '’s', 'context', 'count', 'dip', 'below', 'BPE', '’s', ',', 'a', 'trend', 'which', 'continue', 'all', 'the', 'way', 'through', 'the', 'vocabulary', ',', 'make', 'up', 'a', 'more', 'contextually', 'coherent', 'set', '.', 'these', 'finding', 'hold', 'for', 'English', 'as', 'well', 'as', 'Turkish', ',', 'and', 'replicate', 'when', 'tak-', 'e', 'a', 'context', 'window', 'of', 'size', '2', ',', 'different', 'from', 'that', 'use', 'during', 'SAGE', 'construction', '.', 'figure', '4', ':', 'distribution', 'of', 'token', 'neighbor', '/', 'frequency', 'ratio', 'for', 'a', 'width-5', 'window', 'in', 'English', '(', 'top', ')', 'and', 'Turkish', '(', 'bottom', ')', ';', 'BPE', '(', 'leave', ')', 'and', 'SAGE', '(', 'right', ')', '628', 'these', 'finding', 'can', 'arguably', 'be', 'attribute', 'to', 'a', 'fre-', 'quency', 'artifact', ',', 'where', 'SAGE', 'simply', 'output', 'more', 'token', 'with', 'low', 'frequency', 'in', 'order', 'to', 'provide', 'they', 'with', 'few', 'context', '.', 'we', 'thus', 'present', 'a', 'nor-', 'malize', 'analysis', 'in', 'figure', '4', ',', 'depict', 'the', 'ratio', 'between', 'each', 'token', '’s', 'number', 'of', 'unique', 'neighbor', 'and', 'its', 'frequency', ',', 'distribute', 'over', 'the', 'entire', 'vocab-', 'ulary', '.', 'SAGE', 'provide', 'substantially', 'low', 'ratio', 'in', 'both', 'language', ',', 'support', 'our', 'original', 'claim', '.', '4.1', 'Robustness', 'to', 'Domain', 'Change', 'one', 'possible', 'limitation', 'of', 'the', 'SAGE', 'objective', 'be', 'that', 'it', 'increase', 'the', 'reliance', 'on', 'the', 'original', 'training', 'corpus', 'compare', 'to', 'word', '-', 'count', '-', 'only', 'algorithm', '.', 'in', 'and', 'of', 'itself', ',', 'this', 'should', 'not', 'necessarily', 'be', 'view', 'as', 'a', 'problem', ',', 'assume', 'the', 'collect', 'cor-', 'pus', 'be', 'a', 'faithful', 'representative', 'of', 'an', 'LLM', '’s', 'use', 'case.5', 'to', 'this', 'end', ',', 'we', 'collect', 'comparable', 'cor-', 'pora', 'from', 'non', '-', 'wikipedia', 'domain', 'and', 'run', 'our', 'anal-', 'ysis', 'on', 'the', 'SAGE', 'and', 'BPE', 'vocabulary', 'train', 'on', 'Wikipedia', '.', 'our', 'finding', 'suggest', 'that', 'while', 'SAGE', 'lose', 'its', 'relative', 'advantage', 'in', 'context', '-', 'dependence', 'over', 'BPE', ',', 'it', 'do', 'not', 'fall', 'behind', 'it', '(', 'i.e.', 'it', 'have', 'not', 'overfit', 'to', 'the', 'Wikipedia', 'domain', ')', '.', 'we', 'present', 'a', 'fertility', 'chart', 'for', 'an', 'English', 'corpus', 'of', '7.5', 'M', 'word', 'from', 'Quora', 'questions6', 'in', 'Figure', '5', ',', 'depict', 'simi-', 'lar', 'trend', 'to', 'that', 'on', 'Wikipedia', '(', 'Figure', '2', ')', 'but', 'with', 'small', 'difference', 'between', 'SAGE', 'and', 'BPE', ';', 'the', 'neighbor', '-', 'to', '-', 'frequency', 'ratio', 'aggregation', 'chart', 'in', 'Figure', '6', 'differ', 'from', 'Figure', '4', '(', 'top', ')', 'substantially', 'but', 'show', 'that', 'SAGE', 'and', 'BPE', 'token', 'do', 'not', 'di-', 'verge', 'significantly', 'on', 'this', 'measure', '.', 'we', 'repeat', 'the', 'experiment', 'on', 'english', 'legal', 'text', 'center', 'on', 'US', 'congress', 'bill', '(', 'Henderson', 'et', 'al', '.', ',', '2022', ')', 'and', 'on', 'a', '2.6m', '-', 'word', 'turkish', 'corpus', 'of', 'online', 'reviews,7', 'and', 'observe', 'similar', 'trend', '.', 'these', 'result', 'indicate', 'that', 'while', 'a', 'consider-', 'able', 'amount', 'of', 'the', 'long', 'token', 'prefer', 'by', 'SAGE', 'be', 'select', 'to', 'optimize', 'contextuality', 'in', 'the', 'source', 'domain', ',', 'as', 'it', 'be', 'design', 'to', 'do', ',', 'there', 'be', 'no', '\"', 'short', 'blanket', '\"', 'effect', 'for', 'text', 'originate', 'in', 'different', 'domain', '.', 'this', 'could', 'either', 'be', 'due', 'to', 'wide-', 'scope', 'advantage', 'of', 'some', 'of', 'the', 'token', 'select', 'by', 'SAGE', ',', 'or', 'due', 'to', 'an', 'intrinsic', 'deficiency', 'in', 'BPE', '’s', 'long', '-', 'tail', 'token', ',', 'or', 'a', 'combination', 'of', 'both', '.', '5indeed', ',', 'exist', 'literature', 'recommend', 'add', 'pre-', 'training', 'step', 'on', 'new', 'domain', 'before', 'fine', '-', 'tuning', 'model', 'for', 'they', '(', 'e.g.', ',', 'Han', 'and', 'Eisenstein', ',', '2019', ')', '.', '6https://huggingface.co/datasets/', 'chenghao', '/', 'quora_question', '7https://huggingface.co/datasets/', 'cansen88', '/', 'turkishReviews_5_topic', 'Figure', '5', ':', 'number', 'of', 'subword', 'require', 'to', 'tokenize', 'a', 'word', 'use', 'the', 'original', 'Wikipedia', '-', 'train', 'vocabulary', ',', 'collect', 'over', 'a', 'English', 'Quora', 'question', 'corpus', '.', 'Figure', '6', ':', 'distribution', 'of', 'token', 'neighbor', '/', 'frequency', 'ratio', 'for', 'a', 'width-5', 'window', 'in', 'English', ',', 'base', 'on', 'a', 'Wikipedia', '-', 'train', 'vocabulary', 'and', 'collect', 'over', 'a', 'En-', 'glish', 'Quora', 'question', 'corpus', '.', '5', 'Downstream', 'Evaluation', 'in', 'order', 'to', 'evaluate', 'the', 'utility', 'of', 'our', 'tokenization', 'algorithm', 'for', 'major', 'NLP', 'task', ',', 'we', 'compare', 'SAGE', 'to', 'a', 'BPE', 'vocabulary', 'of', 'the', 'same', 'size', 'by', 'mean', 'of', 'pre', '-', 'train', 'a', 'BERT', '-', 'parameterized', 'model', '(', 'De-', 'vlin', 'et', 'al', '.', ',', '2019', ')', 'use', 'an', 'expedite', 'training', 'scheme', '(', 'Izsak', 'et', 'al', '.', ',', '2021', ')', '.', 'we', 'then', 'evaluate', 'the', 'LLM', '’s', 'performance', 'both', 'on', 'sequence', 'classifica-', 'tion', 'via', 'the', 'English', 'GLUE', 'benchmark', '(', 'Wang', 'et', 'al', '.', ',', '2018', ')', 'and', 'the', 'turkish', 'partition', 'of', 'XNLI', '(', 'Conneau', 'et', 'al', '.', ',', '2018', ')', ',', 'and', 'on', 'name', 'entity', 'recognition', 'in', 'English', '(', 'Wang', 'et', 'al', '.', ',', '2019', ')', 'and', 'turkish', '(', 'Al', '-', 'Rfou', 'et', 'al', '.', ',', '2015', ')', '.', 'we', 'use', 'the', 'default', 'setting', 'from', 'Huggingface', '’s', 'library', 'implementation', 'of', 'the', 'fine-', 'tuning', 'process', '(', 'Wolf', 'et', 'al', '.', ',', '2020', ')', 'and', 'do', 'not', 'perform', 'hyperparameter', 'tune', 'for', 'either', 'model', '.', 'we', 'present', 'our', 'result', 'on', 'sequence', '-', 'level', 'task', 'in', 'Table', '4', '.', 'sage', 'tokenization', 'improve', 'perfor-', 'mance', 'on', 'nearly', 'all', 'task', 'with', 'particularly', 'sub-', '629', 'MRPC', 'MNLI', 'COLA', 'QNLI', 'SST2', 'STSB', 'QQP', 'XNLItur', '(', 'F1', ')', '(', 'acc', '%', ')', '(', 'Matt', '.', ')', '(', 'acc', '%', ')', '(', 'acc', '%', ')', '(', 'Pear', '.', ')', '(', 'acc', '%', ')', '(', 'acc', '%', ')', 'BPE', '.7918', '62.76', '.0777', '66.17', '80.54', '.3094', '82.75', '41.20', 'SAGE', '.8004', '64.00', '.0985', '74.83', '79.85', '.3387', '84.69', '46.46', 'Table', '4', ':', 'performance', 'on', 'sequence', '-', 'level', 'task', 'for', 'BERT', 'model', 'train', 'on', 'different', '16k', '-', 'size', 'vocabulary', '.', 'XNLItur', 'be', 'turkish', ',', 'the', 'rest', 'be', 'english', 'GLUE', 'task', '.', 'all', 'result', 'average', 'over', 'three', 'run', 'on', 'the', 'dev', 'set', 'with', 'different', 'seed', '.', 'English', 'Turkish', 'BPE', '.7142', '.4660', 'SAGE', '.7502', '.5475', 'Table', '5', ':', 'performance', '(', 'f1', ')', 'on', 'NER', 'task', 'of', 'BERT', 'Turk-', 'ish', 'and', 'english', 'model', 'train', 'on', 'different', 'subword', 'vocabulary', 'of', 'size', '16,000', '.', 'all', 'result', 'average', 'over', 'three', 'run', 'on', 'the', 'dev', 'set', 'with', 'different', 'seed', '.', 'stantial', 'improvement', '(', '1.3–8', 'accuracy', 'point', ')', 'on', 'NLI', 'dataset', '.', 'result', 'on', 'NER', 'be', 'present', 'in', 'Table', '5', ',', 'again', 'show', 'SAGE', '’s', 'dominance', 'over', 'BPE', '.', 'due', 'to', 'the', 'length', 'of', 'the', 'training', 'pipeline', 'lead-', 'ing', 'from', 'vocabulary', 'creation', 'through', 'pre', '-', 'train', 'to', 'fine', '-', 'tuning', ',', 'it', 'be', 'difficult', 'to', 'find', 'individual', 'ex-', 'ample', 'where', 'difference', 'in', 'tokenization', 'lead', 'to', 'direct', 'change', 'in', 'prediction', ';', 'we', 'attribute', 'the', 'con-', 'sistent', 'overall', 'gain', 'in', 'downstream', 'performance', 'mostly', 'to', 'the', 'LLM', 'pre', '-', 'training', 'step', ',', 'where', 'the', 'design', 'of', 'SAGE', '’s', 'context', '-', 'friendly', 'vocabulary', 'en-', 'able', 'a', 'more', 'coherent', 'contextual', 'signal', 'to', 'flow', 'through', 'the', 'transformer', 'layer', 'during', 'backpropaga-', 'tion', '.', 'we', 'note', 'that', 'in', 'general', ',', 'our', 'model', 'fare', 'bad', 'on', 'glue', 'task', 'compare', 'to', 'Izsak', 'et', 'al', '.', '(', '2021', ')', '.', 'we', 'attribute', 'this', 'in', 'part', 'to', 'the', 'small', 'token', 'vocab-', 'ulary', 'size', ',', 'and', 'more', 'substantially', 'to', 'the', 'small', 'pre', '-', 'training', 'corpus', 'we', 'use', 'in', 'our', 'experiment', '.', '6', 'Related', 'work', 'in', 'recent', 'year', ',', 'a', 'grow', 'body', 'of', 'research', 'have', 'demonstrate', 'the', 'shortcoming', 'of', 'exist', 'tok-', 'enization', 'algorithm', 'in', 'the', 'context', 'of', 'represent-', 'ing', 'linguistic', 'phenomenon', 'in', 'different', 'language', 'across', 'different', 'task', '(', 'Banerjee', 'and', 'Bhattacharyya', ',', '2018', ';', 'Klein', 'and', 'Tsarfaty', ',', '2020', ';', 'Hakimi', 'Parizi', 'and', 'Cook', ',', '2020', ';', 'Rust', 'et', 'al', '.', ',', '2021', ';', 'Maronikolakis', 'et', 'al', '.', ',', '2021', ';', 'Mielke', 'et', 'al', '.', ',', '2021', ';', 'Hofmann', 'et', 'al', '.', ',', '2021', ')', ',', 'as', 'well', 'as', 'the', 'statistical', 'property', 'affect', 'their', 'downstream', 'performance', '(', 'Bostrom', 'and', 'Durrett', ',', '2020', ')', '.', 'our', 'work', 'address', 'the', 'concern', 'raise', 'in', 'this', 'line', 'of', 'work', 'by', 'introduce', 'an', 'improve', 'sub-', 'word', 'vocabulary', 'creation', 'method', 'which', 'leverage', 'the', 'contextual', 'aspect', 'of', 'the', 'main', 'intend', 'use', 'case', ',', 'namely', 'llm', '.', 'previous', 'work', 'towards', 'this', 'goal', 'include', 'algorithm', 'which', 'offer', 'robustness', 'within', 'an', 'exist', 'subword', 'vocabulary', '(', 'Provilkov', 'et', 'al', '.', ',', '2020', ';', 'he', 'et', 'al', '.', ',', '2020', ';', 'Hiraoka', ',', '2022', ')', ',', 'neces-', 'sitate', 'modification', 'of', 'either', 'training', ',', 'inference', ',', 'or', 'both', 'procedure', 'in', 'the', 'context', 'of', 'llm', '.', 'other', 'have', 'consider', 'tune', 'the', 'size', 'of', 'a', 'subword', 'vo-', 'cabulary', '(', 'Salesky', 'et', 'al', '.', ',', '2020', ')', ',', 'or', 'select', 'from', 'an', 'enlarged', 'set', 'of', 'possible', 'segmentation', '(', 'Asgari', 'et', 'al', '.', ',', '2020', ')', ',', 'for', 'optimize', 'performance', 'on', 'down-', 'stream', 'task', '.', 'some', 'alternative', 'tokenization', 'method', 'focus', 'on', 'the', 'application', 'of', 'a', 'model', 'which', 'consider', 'the', 'expect', 'downstream', 'task', 'together', 'with', 'the', 'pre-', 'training', 'corpus', '(', 'Hiraoka', 'et', 'al', '.', ',', '2020', ')', ',', 'to', 'the', 'de-', 'gree', 'of', 'jointly', 'optimize', 'the', 'tokenizer', 'with', 'the', 'downstream', 'model', '(', 'Hiraoka', 'et', 'al', '.', ',', '2021', ')', '.', 'in', 'ad-', 'dition', 'to', 'the', 'massive', 'change', 'in', 'training', 'and', 'in-', 'ference', 'procedure', 'this', 'approach', 'incur', ',', 'we', 'note', 'that', 'it', 'be', 'difficult', 'to', 'apply', 'to', 'large', 'contextualize', 'model', 'due', 'to', 'the', 'long', 'path', 'from', 'tokenization', 'to', 'prediction', ';', 'SAGE', 'overcome', 'this', 'problem', 'by', '\"', 'nudge', '\"', 'only', 'the', 'LLM', 'vocabulary', 'itself', 'towards', 'a', 'contextualization', '-', 'friendly', 'segmentation', '.', 'the', 'concept', 'of', 'subword', 'tokenization', 'make', 'its', 'rise', 'alongside', 'that', 'of', 'contextualize', 'representa-', 'tion', ',', 'mean', 'that', 'little', 'work', 'exist', 'where', 'skip-', 'GRAM', 'or', 'other', 'static', 'model', 'be', 'train', 'over', 'proper', 'subword', 'segmentation', '.', 'recently', ',', 'Kaushal', 'and', 'Mahowald', '(', '2022', ')', 'do', 'so', 'for', 'a', 'proof', '-', 'of', '-', 'concept', 'of', 'a', 'spelling', 'prediction', 'model', ',', 'in', 'lieu', 'of', 'train', 'full', 'llm', '.', 'to', 'our', 'knowledge', ',', 'no', 'work', 'to', 'date', 'have', 'use', 'a', 'static', 'embed', '-', 'base', 'objective', 'to', 'score', 'token', 'sequence', 'likelihood', 'for', 'a', 'separate', 'task', '(', 'as', 'we', 'do', 'for', 'vocabulary', 'trimming', ')', '.', 'finally', ',', 'we', 'acknowledge', 'the', 'recent', 'effort', 'to', 'do', 'away', 'with', 'tokenization', 'altogether', ',', 'be', 'it', 'through', 'character', '-', 'only', '(', 'Clark', 'et', 'al', '.', ',', '2022', ')', 'or', 'byte-', 'only', '(', 'Xue', 'et', 'al', '.', ',', '2022', ')', 'model', ',', 'or', 'through', 'encode', 'character', 'visually', 'and', 'pass', 'they', 'through', 'a', 'vi-', 'sion', 'model', '(', 'Salesky', 'et', 'al', '.', ',', '2021', ';', 'Rust', 'et', 'al', '.', ',', '2022', ')', '.', '630', 'these', 'represent', 'an', 'even', 'more', 'radical', 'departure', 'from', 'the', 'establish', 'application', 'of', 'LLMs', ',', 'and', 'we', 'look', 'forward', 'to', 'test', 'their', 'ability', 'against', 'our', 'im-', 'prove', 'contextual', 'subword', 'tokenization', 'method', '.', 'we', 'note', 'that', 'while', 'these', 'model', 'have', 'be', 'fac-', 'ing', 'issue', 'regard', 'scaling', ',', 'mostly', 'on', 'the', 'decod-', 'ing', 'side', ',', 'SAGE', 'vocabulary', 'be', 'ready', 'to', 'be', 'use', 'immediately', 'within', 'exist', 'popular', 'LLM', 'imple-', 'mentation', '.', 'furthermore', ',', 'recent', 'work', 'have', 'show', 'the', 'limited', 'utility', 'of', 'character', '-', 'level', 'transformer', 'in', 'semantic', 'task', ',', 'even', 'for', 'morphologically', 'rich', 'language', 'with', 'nontrivial', 'orthography', '-', 'morphology', 'relation', '(', 'Keren', 'et', 'al', '.', ',', '2022', ')', '.', '7', 'conclusion', 'in', 'this', 'work', ',', 'we', 'introduce', 'SAGE', ',', 'a', 'context', '-', 'aware', 'tokenizer', 'build', 'use', 'insight', 'from', 'BPE', ',', 'uni-', 'GRAMLM', ',', 'and', 'SKIPGRAM', ',', 'and', 'show', 'that', 'it', 'achieve', 'well', 'result', 'when', 'use', 'in', 'an', 'LLM', '-', 'pre-', 'train', '-', 'then', '-', 'fine', '-', 'tune', 'schema', 'on', 'two', 'typologically', 'distant', 'language', 'on', 'both', 'the', 'sequence', 'and', 'token', 'level', '.', 'we', 'believe', 'that', 'further', 'investigation', 'into', 'incorporate', 'context', 'in', 'tokenization', 'model', 'can', 'improve', 'result', 'even', 'far', ',', 'and', 'intend', 'to', 'also', 'extend', 'our', 'effort', 'toward', 'other', 'language', 'and', 'writ-', 'ing', 'system', ',', 'as', 'well', 'as', 'to', 'multilingual', 'tokenizer', '.', 'for', 'example', ',', 'we', 'plan', 'to', 'apply', 'SAGE', 'in', 'the', 'con-', 'text', 'of', 'abjad', 'like', 'Hebrew', 'and', 'Arabic', ',', 'as', 'well', 'as', 'language', 'write', 'in', 'alphasyllabarie', 'such', 'as', 'Devanagari', '.', 'within', 'SAGE', 'itself', ',', 'there', 'be', 'room', 'for', 'improve-', 'ment', '.', 'the', 'algorithm', 'be', 'still', 'relatively', 'slow', ',', 'take', 'roughly', 'a', 'day', 'to', 'run', 'on', 'a', 'strong', 'cpu', ',', 'make', 'it', 'dif-', 'ficult', 'to', 'apply', 'to', 'a', 'truly', 'large', 'corpus', ',', 'to', 'start', 'from', 'a', 'large', 'initial', 'vocabulary', ',', 'or', 'to', 'conduct', 'exhaus-', 'tive', 'search', 'over', 'the', 'hyperparameter', '.', 'we', 'intend', 'to', 'keep', 'optimize', 'it', ',', 'and', 'continue', 'evaluation', 'against', 'other', 'subword', 'and', 'character', '-', 'only', 'schema', '.', 'limitation', 'we', 'acknowledge', 'several', 'limitation', 'of', 'SAGE', ',', 'a', 'novel', 'algorithm', 'still', 'in', 'its', 'development', 'stage', '.', 'first', ',', 'scale', 'the', 'vocabulary', 'creation', 'framework', 'up', 'from', 'corpus', '-', 'level', 'unigram', 'statistic', 'to', 'context', 'dependence', 'incur', 'many', 'point', 'where', 'linear', 'fac-', 'tor', 'turn', 'into', 'quadratic', ',', 'and', 'bad', '.', 'we', 'introduce', 'several', 'heuristic', 'to', 'alleviate', 'this', 'issue', 'in', '§', '3', ',', 'how-', 'ever', 'SAGE', 'still', 'take', 'long', 'to', 'train', 'compare', 'to', 'BPE', 'and', 'other', 'tokenizer', ',', 'by', 'roughly', 'a', 'factor', 'of', 'ten', '.', 'while', 'have', 'no', 'effect', 'on', 'downstream', 'pre-', 'training', 'and', 'fine', '-', 'tuning', 'step', ',', 'it', 'do', 'mean', 'hy-', 'perparameter', 'be', 'more', 'difficult', 'to', 'tune', '.', 'second', ',', 'the', 'prohibitive', 'resource', 'require', 'to', 'implement', 'a', 'full', 'LLM', 'pipeline', 'have', 'limit', 'our', 'downstream', 'evaluation', 'setup', 'to', 'ten', 'individual', 'task', 'on', 'two', 'lan-', 'guage', '.', 'ideally', ',', 'as', 'more', 'language', 'with', 'more', 'di-', 'verse', 'script', 'and', 'typological', 'property', 'be', 'exam-', 'ine', ',', 'well', 'generalization', 'can', 'be', 'make', 'about', 'the', 'utility', 'of', 'integrate', 'context', 'into', 'subword', 'tokenizer', 'vocabulary', '.', 'finally', ',', 'we', 'still', 'do', 'not', 'have', 'a', 'well-', 'form', 'theory', 'of', 'integrate', 'multiple', 'domain', ',', 'lan-', 'guage', ',', 'or', 'script', 'together', 'into', 'a', 'single', 'vocabulary', '.', 'this', 'question', 'have', 'interested', 'researcher', 'in', 'recent', 'year', '(', 'e.g.', ',', 'Chung', 'et', 'al', '.', ',', '2020', ';', 'Rust', 'et', 'al', '.', ',', '2021', ';', 'Zhang', 'et', 'al', '.', ',', '2022', ')', ',', 'yet', 'a', 'tokenizer', '-', 'internal', 'solu-', 'tion', '(', 'as', 'oppose', 'to', 'data', 'balance', 'manipulation', ')', 'still', 'seem', 'to', 'have', 'elude', 'the', 'community', '.', 'this', 'question', 'affect', 'sage', 'more', 'than', 'other', 'tokenizer', ',', 'give', 'its', 'reliance', 'on', 'context', ',', 'which', 'change', 'starkly', 'when', 'consider', 'multiple', 'source', 'of', 'text', 'in', 'unison', '.', 'acknowledgment', 'we', 'thank', 'Jacob', 'Eisenstein', 'and', 'Cassandra', 'Jacobs', 'for', 'work', 'on', 'early', 'version', 'of', 'the', 'high', '-', 'level', 'idea', ',', 'and', 'Timo', 'Schick', 'and', 'L¨utfi', 'Kerem', 'Senel', 'for', 'fruitful', 'conversation', 'in', 'early', 'stage', 'of', 'the', 'project', '.', 'we', 'thank', 'Marco', 'Cognetta', ',', 'Michael', 'Elhadad', ',', 'Omer', 'Levy', ',', 'and', 'attendee', 'of', 'ISCOL', '2022', 'for', 'comment', 'and', 'suggestion', 'on', 'more', 'recent', 'version', 'of', 'the', 'work', '.', 'we', 'thank', 'the', 'reviewer', 'for', 'their', 'helpful', 'comment', '.', 'we', 'thank', 'Kaj', 'Bostrom', ',', 'Peter', 'Izsak', ',', 'and', 'Tamar', 'Levy', 'for', 'help', 'we', 'obtain', 'and', 'operate', 'resource', 'for', 'train', 'our', 'model', '.']\n"
     ]
    }
   ],
   "source": [
    "texts = load_documents(files_list)\n",
    "texts = [remove_references(text) for text in texts]\n",
    "stats = get_doc_statistics(texts)\n",
    "\n",
    "# Exibe as estatísticas\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentences_per_doc</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_tokens_per_doc</th>\n",
       "      <th>top_10_tokens</th>\n",
       "      <th>down_10_tokens</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_prepositions</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199</td>\n",
       "      <td>199.0</td>\n",
       "      <td>5056</td>\n",
       "      <td>5056.0</td>\n",
       "      <td>[(the, 224), (of, 152), (in, 125), (to, 117), ...</td>\n",
       "      <td>[(omer, 1), (attendees, 1), (iscol, 1), (sugge...</td>\n",
       "      <td>1397</td>\n",
       "      <td>636</td>\n",
       "      <td>700</td>\n",
       "      <td>[proceeding, of, the, 17th, Conference, of, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_sentences  avg_sentences_per_doc  num_tokens  avg_tokens_per_doc  \\\n",
       "0            199                  199.0        5056              5056.0   \n",
       "\n",
       "                                       top_10_tokens  \\\n",
       "0  [(the, 224), (of, 152), (in, 125), (to, 117), ...   \n",
       "\n",
       "                                      down_10_tokens  num_nouns  num_verbs  \\\n",
       "0  [(omer, 1), (attendees, 1), (iscol, 1), (sugge...       1397        636   \n",
       "\n",
       "   num_prepositions                                             lemmas  \n",
       "0               700  [proceeding, of, the, 17th, Conference, of, th...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_stats = pd.DataFrame([stats])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentences: 457\n",
      "avg_sentences_per_doc: 457.0\n",
      "num_tokens: 4405\n",
      "avg_tokens_per_doc: 4405.0\n",
      "top_10_tokens: [('vocabulary', 73), ('sage', 71), ('tokens', 52), ('bpe', 50), ('al', 44), ('et', 42), ('association', 38), ('token', 38), ('size', 37), ('v', 35)]\n",
      "down_10_tokens: [('loader', 1), ('dist', 1), ('seq', 1), ('strategy', 1), ('accumulation', 1), ('eval', 1), ('rate', 1), ('grad', 1), ('scheduler', 1), ('polynomial', 1)]\n",
      "num_nouns: 1691\n",
      "num_verbs: 653\n",
      "num_prepositions: 31\n",
      "lemmas: ['Proceedings', '17th', 'Conference', 'European', 'Chapter', 'Association', 'Computational', 'Linguistics', ',', 'page', '623–635', '2', '-', '6', ',', '2023', '©', '2023', 'Association', 'Computational', 'Linguistics', 'incorporate', 'Context', 'Subword', 'Vocabularies', 'Shaked', 'Yehezkel', 'Blavatnik', 'School', 'Computer', 'Science', 'Tel', '-', 'Aviv', 'University', 'Tel', '-', 'Aviv', ',', 'Israel', 'shakedy@mail.tau.ac.il', 'Yuval', 'Pinter', 'Department', 'Computer', 'Science', 'Ben', '-', 'Gurion', 'University', 'Negev', 'Beer', 'Sheva', ',', 'Israel', 'uvp@cs.bgu.ac.il', 'abstract', 'current', 'popular', 'subword', 'tokenizer', 'train', 'base', 'word', 'frequency', 'statistic', 'corpus', ',', 'consider', 'informa-', 'tion', 'co', '-', 'occurrence', 'context', '.', 'Neverthe-', ',', 'result', 'vocabulary', 'lan-', 'guage', 'model', \"'\", 'highly', 'contextualize', 'setting', '.', 'present', 'SAGE', ',', 'tokenizer', 'tailor', 'sub-', 'word', 'downstream', 'use', 'bake', 'contextualize', 'signal', 'vocabulary', 'cre-', 'ation', 'phase', '.', 'sage', 'bet-', 'ter', 'job', 'current', 'widespread', 'tokenizer', 'keep', 'token', 'contexts', 'cohesive', ',', 'in-', 'curre', 'large', 'price', 'term', 'encode', 'effi-', 'ciency', 'domain', 'robustness', '.', 'SAGE', 'improve', 'performance', 'English', 'GLUE', 'classification', 'task', 'NER', ',', 'Inference', 'NER', 'Turkish', ',', 'demonstrate', 'robustness', 'language', 'property', 'morphological', 'exponence', 'agglutination', '.', '1', 'introduction', 'research', 'space', 'current', 'NLP', 'focus', 'advance', 'model', ':', 'modify', 'pre-', 'training', 'objective', ',', 'improve', 'network', 'architec-', 'ture', ',', 'add', 'task', 'scheme', 'downstream', 'evaluation', '.', 'limited', 'work', 'dedicated', 'crucial', 'step', 'underlie', 'modern', 'large', 'language', 'model', '(', 'LLMs', ')', ',', 'tokenization', 'phase', '.', 'order', 'process', 'give', 'string', 'text', ',', 'LLM', 'obtain', 'vector', 'representation', 'input', 'seg-', 'menting', 'tokens', '.', '-', '-', 'vocabulary', '(', 'OOV', ')', 'item', 'inhibit', 'performance', 'model', ',', 'current', 'tokenizer', 'produce', 'token', 'possi-', 'bly', 'proper', 'subsegment', 'input', 'word', ',', 'know', 'subword', '.', 'method', ',', 'popularize', 'system', 'WordPiece', '(', 'Schuster', 'Nakajima', ',', '2012', ')', ',', 'Byte', '-', 'Pair', 'Encoding', '(', 'BPE', ';', 'Sennrich', 'et', 'al', '.', ',', '2016', ')', 'UNIGRAMLM', '(', 'Kudo', ',', '2018', ')', ',', 'allow', 'word', 'represent', 'token', ',', 'remove', 'OOV', 'problem', 'allow', 'flexibility', 'determine', 'token', 'vocabulary', 'size', ',', 'ultimately', 'affect', 'model', 'speed', '(', 'BPE', 'son', 'Raj', 'ash', 'ri', 'Sud', 'h', 'ak', 'ar', 'p', 'enn', 'ed', 'dial', 'og', 'ue', 'song', 'film', 'dub', 'Telugu', '.', 'SAGE', 'son', 'Raj', 'ash', 'r', 'Sud', 'h', 'k', 'r', 'penn', 'e', 'd', 'dial', 'ogue', 'song', 'film', 'dub', 'Telugu', '.', 'BPE', 'gene', 'pseud', 'og', 'ene', 'human', 'prim', 'ate', '.', 'SAGE', 'gene', 'pseud', 'ogene', 'human', 'prim', 'ate', '.', 'BPE', 'St', 'o', 'og', 'es', 'work', 'Mir', 'acle', 'Det', 'ective', 'Agency', ',', 'SAGE', 'St', 'o', 'o', 'g', 'e', 's', 'work', 'Mir', 'acle', 'Det', 'ective', 'Agency', ',', 'Table', '1', ':', 'token', 'og', 'select', 'BPE', '(', 'vocabulary', 'size', '16,000', ')', 'achieve', 'frequency', 'objective', ',', 'discard', 'SAGE', 'fail', 'contextually', 'coherent', '.', 'example', 'corpus', 'demonstrate', 'different', 'context', '.', 'softmax', 'generation', 'target', ')', 'performance', '(', 'well', 'ability', 'represent', '-', 'frequent', 'word', ')', '.', 'potential', 'pitfall', 'BPE', 'uni-', 'GRAMLM', ',', 'propose', 'variant', '(', 'et', 'al', '.', ',', '2020', ';', 'Provilkov', 'et', 'al', '.', ',', '2020', ')', ',', 'train', 'word', 'frequency', 'statistic', ',', 'consider', 'information', 'word', 'co', '-', 'occurrence', 'context', '.', 'time', ',', 'result', 'vocabu-', 'larie', 'highly', 'contextualize', 'setting', ',', 'llm', ',', 'single', 'subword', 'og', 'ap-', 'pear', 'different', 'context', 'derive', 'word', 'like', 'dial', 'og', 'ue', 'pseud', 'og', 'ene', '.', 'propose', 'sys-', 'tem', 'prepare', 'subword', 'downstream', 'use', 'bake', 'contextualize', 'signal', 'vocabulary', 'creation', 'step', '.', 'model', ',', 'SAGE', ',', 'use', 'skipgram', 'objective', '(', 'Mikolov', 'et', 'al', '.', ',', '2013', ')', 'corpus', 'basis', 'iteratively', 'eliminate', 'candidate', 'subword', 'initial', 'large', 'vocabulary', 'desire', 'vocabulary', 'size', 'reach', '.', 'table', '1', 'show', ',', 'SAGE', 'succeed', 'remove', 'ambiguous', 'og', 'token', ',', 'facilitate', 'distinct', 'contex-', 'tualization', 'procedure', 'example', 'sentence', '623', '(', 'take', 'Wikipedia', ')', '.', 'present', 'algorithm', ',', 'SAGE', ',', 'pred-', 'icate', 'iterative', 'pruning', 'contextually', 'noisy', 'tokens', 'vocabulary', ',', 'compare', 'effect', 'token', 'property', 'context', 'cohesion', 'BPE', 'in-', '-', '-', 'domain', ',', 'english', 'Turkish', '.', 'evaluate', 'performance', 'downstream', 'task', 'train', 'BERT', '-', 'base', 'LLM', '(', 'Devlin', 'et', 'al', '.', ',', '2019', ')', 'vocabulary', 'produce', 'tokeniz-', 'er', 'language', ',', 'demonstrate', 'substantial', 'improvement', 'english', 'GLUE', 'task', 'NER', ',', 'turkish', 'NLI', 'NER', '.', 'em-', 'phasize', 'oppose', 'current', 'tokenizer', 'variant', ',', 'model', '\"', 'plug', 'play', '\"', 'substitu-', 'tion', 'subword', 'token', 'vocabulary', ',', 'require', 'modification', 'inference', 'protocol', '(', 'code', ')', 'pre', '-', 'training', 'apply', 'applicable', 'LLM', 'popular', 'share', 'library.1', '2', 'Subword', 'Vocabulary', 'Creation', 'method', 'tokenize', 'corpus', 'order', 'later', 'assign', 'token', 'continuous', 'vector', ',', 'em-', 'bedding', ',', 'evolve', 'year', '.', 'initially', ',', 'word', 'corpus', 'assign', 'em-', 'bed', '(', 'Collobert', 'Weston', ',', '2008', ';', 'Mikolov', 'et', 'al', '.', ',', '2013', ')', '.', 'oov', ',', 'i.e.', 'word', 'appear', 'original', 'training', 'corpus', 'certain', 'fre-', 'quency', 'threshold', ',', 'receive', 'special', '(', 'iden-', 'tical', ')', '\"', 'UNK', '\"', 'vector', '.', 'Subword', 'tokenizer', '(', 'Schuster', 'Nakajima', ',', '2012', ';', 'Wu', 'et', 'al', '.', ',', '2016', ')', 'intro-', 'duce', 'alleviate', 'issue', ',', 'allow', 'segmentation', 'text', 'embeddable', 'unit', '(', 'assume', 'un-', 'see', 'character', ',', 'relaxed', 'constraint', 'language', 'alphabetical', 'script', ')', '.', 'training', 'process', 'create', 'subword', 'vocabulary', 'model', 'decode', 'text', 'input', 'involve', 'optimize', 'encode', 'objective', 'large', 'cor-', 'pus', '.', 'date', ',', 'tokenizer', 'practice', 'large', 'model', 'focus', 'efficiency', 'information-', 'theoretic', 'objective', ',', 'reduce', 'corpus', 'unigram', 'frequency', 'count', 'space', '-', 'delimit', 'word', ',', 'reduce', 'calculation', 'time', 'lose', 'contextual', 'signal', '.', 'sage', 'reintroduce', 'contextual', 'depen-', 'dencie', 'word', 'vocabulary', 'creation', '-', 'stage', 'process', ',', '-', 'application', 'BPE', 'follow', 'iterative', 'pruning', 'idea', 'inspire', 'UNIGRAMLM', 'SKIPGRAM', '.', 'briefly', 'present', 'algorithm', 'tie', 'SAGE', '.', '1our', 'code', 'model', 'available', 'www.github', '.', 'com', '/', 'MeLeLbgu', '/', 'SaGe', '.', 'Algorithm', '1', 'Byte', '-', 'pair', 'encode', 'vocabulary', 'cre-', 'ation', '(', 'Gage', ',', '1994', ';', 'Sennrich', 'et', 'al', '.', ',', '2016', ')', 'input', ':', 'Corpus', 'C', ',', 'Vocabulary', 'final', 'size', 'v', '.', 'output', ':', 'Vocabulary', 'v', 'size', 'v', '(', 'order', ')', '.', '1', ':', 'procedure', 'bpe(c', ',', 'v', ')', '2', ':', 'v', '←all', 'unique', 'character', 'C', '3', ':', '|v|', '<', 'v', '▷merge', 'token', '4', ':', '⟨tl', ',', 'tr⟩←most', 'frequent', 'bigram', 'C', '5', ':', 'tnew', '←tL', '⊕tr', '▷make', 'new', 'token', '6', ':', 'V', '←V', '⊕[tNEW', ']', '7', ':', 'c.replaceall(⟨tl', ',', 'tR⟩', ',', 'tNEW', ')', '8', ':', 'end', '9', ':', 'return', 'v', '10', ':', 'end', 'procedure', 'Byte', '-', 'Pair', 'Encoding', '.', 'BPE', 'algorithm', 'cre-', 'ates', 'vocabulary', '\"', '-', '\"', ',', 'start', 'sin-', 'gle', 'character', 'alphabet', ',', 'iteratively', 'add', 'token', 'reach', 'desire', 'vocabulary', 'size', '.', 'iteration', ',', 'add', 'token', 'concate-', 'nation', 'frequent', 'adjacent', 'pair', 'ex-', 'isting', 'token', '(', 'Algorithm', '1', ')', '.', 'default', 'set-', 'ting', 'algorithm', 'popular', 'implementa-', 'tion', '(', 'Kudo', 'Richardson', ',', '2018', ')', 'restrict', 'token', 'addition', 'word', 'boundary', ',', 'facilitate', 'train-', 'ing', 'unigram', 'frequency', '.', 'addition', ',', 'LLM', 'tokenizer', 'BPE', '(', 'Liu', 'et', 'al', '.', ',', '2019', ';', 'Radford', 'et', 'al', '.', ',', '2019', ';', 'Wolf', 'et', 'al', '.', ',', '2020', ')', 'decode', 'sequence', 'apply', 'merge', 'order', 'vocabulary', ',', 'originally', 'dictate', 'algorithm', ',', 'greedy', 'large', '-', 'subsequence', 'left', '-', '-', 'right', 'inference', '.', 'Unigram', 'Language', 'Model', '.', 'UNIGRAMLM', 'of-', 'fer', '-', 'vocabulary', 'creation', 'process', ',', 'start-', 'ing', 'initial', 'vocabulary', 'substring', 'input', 'corpus', 'pruning', 'token', 'iteratively', 'reach', 'desire', 'vocabulary', 'size', '.', 'pruning', 'procedure', 'involve', 'calculate', 'overall', 'unigram', 'likelihood', 'corpus', 'current', 'vocabulary', 'versus', 'vocabulary', 'lacking', 'candidate', 'pruning', 'token', '(', 'Algorithm', '2', 'detail', ')', ',', 'refer', 'ablation', 'objective', '.', 'system', ',', 'decoding', 'ideally', 'perform', 'consider', 'prob-', 'ability', 'possible', 'segmentation', ',', 'e.g.', ',', 'Viterbi', 'algorithm', ';', ',', 'common', 'practice', 'use', 'leave', '-', '-', 'right', 'greedy', 'decoding', '.', 'Skipgram', 'Objective', '.', 'SKIPGRAM', 'objec-', 'tive', '(', 'Mikolov', 'et', 'al', '.', ',', '2013', ')', 'formalize', 'relation', 'target', 'token', 't', 'context', ',', 'ask', 'context', 'token', 'c', 'window', 'Wt', 'pre', '-', 'define', 'size', 'predict', 't.', 'prediction', 'sigmoid', 'activation', 'inner', 'product', 'embedding', 'train', 'target', '(', 'e(t', ')', ')', 'contexts', '(', 'E(C', ')', ')', '.', 'aggregate', '624', 'Algorithm', '2', 'UNIGRAMLM', 'vocabulary', 'creation', '(', 'Kudo', ',', '2018', ')', '.', 'n', 'arg', 'minx', 'denote', 'n', 'bottom-', 'rank', 'element', 'X.', 'input', ':', 'Corpus', 'C', ',', 'Vocabulary', 'final', 'size', 'v', ',', 'pruning', 'batch', 'size', 'k.', 'output', ':', 'Vocabulary', 'v', 'size', 'v', '.', '1', ':', 'procedure', 'unigramlm(c', ',', 'v', ')', '2', ':', 'v', '←all', 'substring', 'occur', 'c', '3', ':', '|v|', '>', 'V', '▷Prune', 'token', '4', ':', 'x(j', ')', '←tokenize(C', ',', 'v', ')', '5', ':', 'l(v', ')', '←', '|C|', 'X', 'j=1', 'log', '\\x10', 'p(x(j', ')', ')', '\\x11', '6', ':', 't', '∈v', ':', '▷calculate', 'ablation', 'objective', '7', ':', 'losst', '←L(V', '\\\\', '{', 't', '}', ')', '−l(v', ')', '8', ':', 'end', '9', ':', 'P', '←min(k', ',', '|V|', '−V', ')', 'arg', 'mint∈V(losst', ')', '10', ':', 'V', '←V', '\\\\', 'P', '▷Prune', '11', ':', 'end', '12', ':', 'return', 'v', '13', ':', 'end', 'procedure', 'token', 'corpus', ',', 'skipgram', 'total', 'likelihood', 'measure', ',', 'approximate', 'overall', 'contextual', 'cohesion', ':', 'l(v', ',', 'C', ')', '=', '−', 'X', 't∈tok(C', ',', 'V', ')', 'X', 'cj∈wt', 'log', '\\x10', 'σ(E(T', ')', 't', '·', 'E(C', ')', 'cj', ')', '\\x11', '.', '(', '1', ')', 'token', 'vocabulary', 'inference', 'method', 'change', ',', 'target', 'sequences', 'con-', 'text', ',', 'result', 'difference', 'aggregate', 'likeli-', 'hood', 'act', 'score', 'compare', 'tokenization', '.', 'use', 'behavior', 'ablation', 'objective', 'SAGE', '.', '3', 'SAGE', 'Vocabulary', 'Creation', 'SAGE2', '-', 'tokenizer', ',', 'follow', 'UN-', 'IGRAMLM', 'general', 'procedure', ',', 'incorporate', 'skipgram', 'objective', 'vocabulary', 'trim', 'rule', '.', 'give', 'initial', 'vocabulary', 'v', 'corpus', 'C', ',', 'SAGE', 'compute', 'skipgram', 'embed', 'space', 'v', 'provide', 'overall', 'likelihood', 'C', '(', '1', ')', '.', 'proceed', 'calculate', 'loss', 'token', 'vocabulary', 'remove', ',', 'eliminate', 'token', 'incur', 'minimal', 'loss', '-', 'tokenize', 'corpus', 'accord', 'update', 'vocabulary', ',', 'repeat', 'procedure', 'reach', 'desire', 'vocabulary', 'size', 'v', '.', 'having', 'learn', 'vocabulary', ',', 'downstream', 'inference', 'pro-', 'ceed', 'exactly', 'segmentation', '-', 'base', 'method', ',', 'greedy', 'left', '-', '-', 'right', 'manner', '.', 'SAGE', 'adapt', 'anticipate', 'decode', '2The', 'acronym', ';', 'intend', 'evoke', 'SkipGram', 'maintain', '\"', 'suffix', '\"', 'BPE', '.', 'Algorithm', '3', 'sage', 'vocabulary', 'creation', '.', 'n', 'arg', 'minx', 'denote', 'n', '-', 'rank', 'element', 'X.', 'input', ':', 'Corpus', 'C', ',', 'Vocabulary', 'final', 'size', 'v', ',', 'basic', 'tok-', 'enizer', 'T', ',', 'overshoot', 'factor', 'n', ',', 'pruning', 'batch', 'size', 'k', ',', 'likelihood', 'recalculation', 'frequency', 'm', ',', 'size', 'pruning', 'candidate', 'set', 'M', ',', 'embed', 'recalculation', 'frequency', 'l.', 'output', ':', 'Vocabulary', 'v', 'size', 'v', '.', '1', ':', 'procedure', 'sage(c', ',', 'v', ')', '2', ':', 'V', '←T', '(', 'C', ',', 'n', '·', 'v', ')', '3', ':', '←0', '4', ':', '|v|', '>', 'v', '5', ':', '≡0', '(', 'mod', 'l', '×', 'm', ')', '6', ':', 'EV', '←Word2Vec(V', ')', '▷embedde', 'table', '7', ':', 'end', '8', ':', 'l(v', ',', 'C', ')', '←SGObj(EV', ',', 'C', ')', '▷total', 'likelihood', '(', '1', ')', '9', ':', '≡0', '(', 'mod', 'm', ')', '▷update', 'set', '10', ':', 't', '∈v', ':', '11', ':', 'losst', '←L(V', '\\\\', '{', 't', '}', ',', 'C', ')', '−l(v', ',', 'C', ')', '12', ':', 'end', '13', ':', 'Vbot', '←m', 'arg', 'mint∈V(losst', ')', '14', ':', '▷update', 'loss', 'set', '15', ':', 't', '∈vbot', ':', '16', ':', 'losst', '←L(V', '\\\\', '{', 't', '}', ',', 'C', ')', '−l(v', ',', 'C', ')', '17', ':', 'end', '18', ':', 'end', '19', ':', 'P', '←min(k', ',', '|V|', '−V', ')', 'arg', 'mint∈vbot(losst', ')', '20', ':', 'Vbot', '←Vbot', '\\\\', 'P', '▷Prune', '21', ':', 'V', '←V', '\\\\', 'P', '22', ':', '←i', '+', '1', '23', ':', 'end', '24', ':', 'return', 'v', '25', ':', 'end', 'procedure', 'algorithm', ',', 'change', '-', 'tokenization', 'step', 'accordingly', '.', 'practice', ',', 'apply', 'process', 'describe', 'introduce', 'multiple', 'source', 'considerable', 'computational', 'complexity', ':', 'example', ',', 'calculat-', 'ing', 'ablation', 'objective', 'token', 'it-', 'eration', 'produce', 'quadratic', 'calculation', 'entire', 'corpus', ';', 'recalculate', 'embedding', 'update', 'vocabulary', 'similarly', 'unreason-', 'able', 'perform', 'iteration', '.', 'ameliorate', 'source', 'complexity', 'se-', 'rie', 'heuristic', 'find', 'preliminary', 'experiment', 'minimally', 'disruptive', 'precision', 'likeli-', 'hood', 'calculation', '.', 'describe', 'heuristic', ',', 'depict', 'Algorithm', '3', '.', ',', 'in-', 'stead', 'initialize', 'vocabulary', 'set', 'possible', 'character', 'sequence', 'corpus', ',', 'UNIGRAMLM', ',', 'use', 'exist', 'noncontextual', 'tokenizer', 'BPE', 'learn', 'vocabulary', 'large', 'v', 'factor', 'n', ',', 'begin', 'prune', 'pro-', 'cess', '.', ',', 'instead', 'remove', 'single', 'token', 'loss', '-', 'rank', 'vocab-', 'ulary', ',', 'remove', 'batch', 'k', 'tokens', '625', 'Sentence', 'fragment', '.', '.', '.', 'use', 'include', 'directive', 'refer', '.', '.', '.', 'Tokenization', 'V', 'use', 'includ', '[', 'e', 'direct', 've', ']', 'ref', 'er', 'r', 'ing', 'Tokenization', 'V', '\\\\', '{', 'includ', '}', 'use', 'inc', 'l', 'u', '[', 'de', 'direct', 've', ']', 'ref', 'er', 'r', 'ing', 'Table', '2', ':', 'effect', 'retokenization', 'context', 'window', 'width', '2', '(', 'bracket', ')', 'surround', 'target', 'token', '(', 'bold', ')', '.', 'leave', '-', 'context', 'token', 'replace', 'result', '-', '-', 'window', 'vocabulary', 'ablation', '.', 'time', ',', 'unigramlm.3', 'avoid', 'fre-', 'quent', 'loss', 'recalculation', ',', 'recompute', 'entire', 'likelihood', 'set', 'm', 'ablation', 'step', ',', 'M', 'token', 'prune', 'candidate', 'm', 'step', '.', 'preliminary', 'experiment', 'support', 'decision', ',', 'find', 'rank', 'list', 'loss', 'tend', 'stay', 'relatively', 'stable', 'dozen', 'batch', '-', 'prune', 'iteration', '.', 'lastly', ',', 'avoid', 'costly', '-', 'training', 'embed', 'matrix', 'token', 'give', 'update', 'corpus', ',', 'result', 'minor', 'change', 'likelihood', 'subsequent', 'iter-', 'ation', ',', 'perform', 'l', 'iteration', 'batch', ',', 'i.e.', 'ablation', 'k', '×', 'm', '×', 'l', 'tokens', '.', 'n', ',', 'k', ',', 'l', ',', 'm', 'M', 'algorithm', 'hyperparameter', 'tune', 'empirically', 'base', 'desire', 'runtime', ',', 'corpus', 'size', 'vocabulary', 'size', '.', 'Contextual', 'Loss', '.', 'order', 'calculate', 'per-', 'token', 'skipgram', 'likelihood', 'loss', ',', 'sentence', 'token', 't', 'occur', 'need', '-', 'segment', 'accord', 'V', '\\\\', '{', 't', '}', ',', 'new', 'likelihood', 'record', '.', 'support', 'perform', 'calculation', 'large', 'scale', ',', 'maintain', 'mapping', 'token', 'sentence', 'contain', ',', 'sen-', 'tence', \"'\", 'current', 'likelihood', '.', 'sentence', 'level', 'window', 'level', ',', 'remain', 'suffix', '-', '-', 'window', '-', 'tokenization', 'combine', '-', 'window', 'char-', 'acter', 'form', 'different', 'token', 'sequence', 'replace-', 'ment', 'give', 'stage', '.', 'consider', 'example', 'Table', '2', ',', '-', 'tokenization', 'result', 're-', 'placement', 'context', 'token', 'distant', 'target', '.', 'Negative', 'Sampling', '.', 'original', 'skipgram', 'objective', 'use', 'negative', 'sample', 'estimate', 'con-', 'text', 'probability', '.', 'application', 'skip-', 'GRAM', 'vocabulary', 'creation', 'algorithm', '(', 'independent', 'embedding', 'training', 'proce-', 'dure', ')', 'include', 'likelihood', 'estimation', 'parameter', 'update', ',', 'sample', 'negative', 'to-', 'ken', ',', 'process', 'introduce', 'substantial', 'noise', 'complexity', '.', '3as', 'UNIGRAMLM', 'ablation', '-', 'base', 'vocabu-', 'larie', ',', 'single', '-', 'character', 'token', 'remove', 'vocabulary', ',', 'order', 'allow', '-', 'alphabet', 'word', 'tokenize', '.', '4', 'SAGE', 'Vocabulary', 'Properties', 'analysis', 'modify', 'algorithm', 'advan-', 'tage', ',', 'train', 'vocabulary', 'pre', '-', 'determined', 'size', 'BPE', 'SAGE', '.', 'select', '|v|', '=', '16', ',', '000', ',', 'obtain', 'corpora', 'English', '(', '750,000', 'line', 'August', '2022', 'Wikipedia', 'dump', ')', 'turkish', '(', 'entire', 'text', 'September', '2022', 'Wikipedia', 'dump', ')', ',', 'opt', 'language', 'share', 'Latin', 'alphabet', 'differ', 'family', '(', 'indo-', 'european', 'vs.', 'Turkic', ')', ',', 'crucially', ',', 'morpholog-', 'ical', 'property', ':', 'english', 'low', '-', 'exponence', ',', 'low-', 'synthesis', 'language', ',', 'turkish', 'feature', 'multiple', 'inflectional', 'exponence', 'high', 'verbal', 'synthesis', ',', 'highly', 'agglutinative', 'morphology', '(', 'Bickel', 'Nichols', ',', '2013a', ',', 'b', ')', '.', 'follow', 'hy-', 'perparameter', 'setting', 'compute', 'vocabulary', ':', 'initial', 'vocab', 'size', '20', ',', '000', '(', 'n', '=', '1.25', ')', ',', 'l', '=', '4', ',', 'k', '=', '100', ',', 'M', '=', '1500', ',', 'm', '=', '10', '.', 'Gen-', 'sim', 'package', 'train', 'skipgram', 'model', '(', 're-', 'hurek', 'Sojka', ',', '2011', ')', ',', 'Sentencepiece', '(', 'Kudo', 'Richardson', ',', '2018', ')', 'obtain', 'initial', 'BPE', 'vocabularies.4', 'hyperparameter', 'detail', 'Appendix', 'a.', 'present', 'analysis', 'result', 'vocabu-', 'larie', ',', 'highlight', 'advantage', 'trade', '-', 'off', 'exhibit', 'context', '-', 'base', 'subword', 'tokenization', '.', 'generally', 'speak', ',', 'token', 'discard', 'SAGE', 'initial', 'vocabulary', 'appear', 'base-', 'line', 'BPE', 'final', 'vocabulary', '.', 'differ-', 'ence', 'vocabulary', 'short', 'to-', 'ken', 'appear', 'BPE', 'SAGE', ',', 'proper', 'substring', 'long', 'token', 'appear', 'BPE', 'vocabulary', '.', 'BPE', '-', 'merge', 'table', 'construction', ',', 'force', 'retention', 'entire', 'chain', 'token', 'create', ':', 'vocabulary', ',', 'th', '.', 'essential', 'original', 'intend', 'decode', 'process', ',', 'actual', 'implementation', 'greedy', 'decod-', 'ing', 'need', 'property', '.', 'SAGE', 'ini-', 'tial', 'vocabulary', 'share', 'characteristic', ',', 'trim', 'process', 'allow', 'token', 'ablated', ',', '4since', 'BPE', 'augment', 'vocabulary', 'iteratively', ',', 'base-', 'line', 'BPE', 'vocabulary', 'proper', 'subset', 'initialize', 'SAGE', '.', '626', 'figure', '1', ':', 'token', 'length', 'distribution', 'BPE', 'vocabulary', 'vs.', 'SAGE', 'English', '.', 'frequent', 'SAGE', 'BPE', 'e', 's', 'es', 'ic', 'ing', 'ist', 'ing', 'ff', 'ation', 'eat', 'ation', 'ate', 'Table', '3', ':', 'Tokens', 'high', 'difference', 'frequency', 'be-', 'tween', 'tokenization', '(', 'english', 'model', ')', '.', 'include', 'middle', 'merge', 'chain', '.', 'an-', 'difference', 'find', 'vocabulary', 'strong', 'preference', 'SAGE', 'word', '-', 'initial', 'token', '.', '83', '%', 'token', 'appear', 'SAGE', 'vocabulary', 'BPE', 'word', '-', 'initial', ',', 'com-', 'pare', '22', '%', 'BPE', '-', 'tokens', '.', 'reasonable', ',', 'token', 'survive', 'sage', 'abla-', 'tion', 'step', 'exhibit', 'high', 'loss', 'condition', 'removal', ',', 'arguably', 'case', 'nearby', 'target', 'word', 'need', 'predict', 'word', '-', 'initial', 'context', '.', 'Token', 'Length', '.', 'figure', '1', 'show', 'histogram', 'token', 'length', '(', 'character', ')', '16,000', '-', 'token', 'SAGE', 'BPE', 'vocabulary', 'English', '(', 'result', 'turkish', 'similar', ')', '.', 'SAGE', 'clearly', 'select', 'long', 'tokens', 'vocabulary', ',', 'sensible', 'outcome', 'give', 'high', 'chance', 'contextually', 'coherent', '.', 'difference', 'stark', 'token', 'length', '2', '3', ';', 'consider', 'token', 'appear', 'exactly', 'final', 'vocabulary', ',', 'find', '56', '%', 'BPE', '-', 'tokens', 'length', '2', '3', ',', '55', '%', 'SAGE', '-', 'tokens', 'length', '5', '.', 'Token', 'Frequency', '.', 'compute', 'frequency', 'token', 'encode', 'form', 'English', 'train-', 'ing', 'corpus', ',', 'SAGE', 'vocabulary', 'Figure', '2', ':', 'number', 'subword', 'require', 'tokenize', 'word', ',', 'collect', 'original', 'english', 'training', 'cor-', 'pus', '.', 'BPE', '.', 'table', '3', 'token', 'big', 'difference', 'frequency', 'be-', 'tween', 'SAGE', 'BPE', 'tokenization', '.', 'SAGE', 'revert', 'single', '-', 'character', 'token', 'consider-', 'ably', 'BPE', '(', 'demonstrate', 'example', 'table', '1', ')', '.', 'view', 'fea-', 'ture', 'context', '-', 'base', 'tokenization', '—', 'vocabulary', 'partition', '(', 'short', ')', 'token', 'highly', 'ambiguous', 'context', '(', 'long', ')', 'to-', 'ken', 'coherent', 'contexts', '.', 'time', ',', 'BPE', 'rife', 'tokens', 'medially', 'ambigu-', 'ous', 'contextually', ',', 'result', 'embedding', 'useful', 'completely', 'ignorable', ',', 'add', 'noise', 'representation', 'sequence', '.', 'result', ',', 'SAGE', 'break', 'complex', 'suffix', ',', 'English', 'compositional', ',', 'constituent', 'morphology', '.', 'suffix', 'ings', 'dismantle', 'ing', 's', ',', 'BPE', 'reserve', 'token', ',', 'unhelpful', '.', 'Subword', 'Fertility', '.', 'Fertility', ',', 'define', 'statistical', 'machine', 'translation', 'literature', ',', 'refer', 'average', 'number', 'subword', 'produce', 'to-', 'kenize', 'word', '.', 'figure', '2', 'exhibit', 'histogram', 'English', 'corpus', 'word', 'subword', 'length', ',', 'BPE', 'vocabulary', 'SAGE', 'vocabu-', 'lary', '.', 'sage', 'retain', 'word', 'single', 'token', ',', 'trade', 'word', 'hav-', 'ing', 'subword', ',', 'compare', 'BPE', 'abundance', 'word', '2', '3', 'subword', '.', 'follow', 'trend', 'describe', 'far', ',', 'sage', 'pref-', 'erence', 'dismantle', 'unknown', 'word', 'mean-', 'ingless', 'single', '-', 'character', 'token', 'confus-', 'ing', ',', 'ambiguous', 'length-2', 'length-3', 'token', '.', 'believe', 'BPE', 'behavior', 'harm', 'text', 'understand-', '627', 'figure', '3', ':', 'number', 'distinct', 'neighbor', 'token', 'encounter', 'width-5', 'window', ',', '200', ',', 'turkish', '.', 'ing', 'suggest', 'ambiguous', 'fragment', '(', 'consider', '\"', 'og', '\"', ')', 'mean', 'LLM', 'try', 'learn', ',', 'SAGE', 'single', '-', 'character', 'breakdown', 'indicate', 'word', 'truly', 'unknown', 'infer', 'compose', 'constituent', '-', 'vocab', 'subword', '.', 'Fertility', 'translate', 'trade', '-', 'encode', 'efficiency', 'SAGE', 'contextual', 'advantage', ':', 'sam-', 'ple', '150', 'K', 'line', 'English', 'Wikipedia', 'en-', 'code', '4', 'million', 'BPE', 'token', ',', 'optimize', 'information', '-', 'theoretic', 'objective', ',', 'SAGE', 'produce', '4.5', 'million', '.', 'having', 'say', ',', 'inef-', 'ficiency', 'offset', 'LLM', 'pre-', 'training', ':', 'propose', 'contextually', 'coherent', 'token', 'require', 'few', 'update', 'step', 'order', 'achieve', 'useful', 'embed', 'parameter', ',', 'help', 'model', 'converge', 'fast', 'compare', 'BPE', 'token', '.', 'leave', 'testing', 'hypothesis', 'future', 'work', '.', 'Contextual', 'Exponence', '.', 'determine', 'degree', 'SAGE', 'effectively', 'optimize', 'token', \"'\", 'con-', 'textual', 'soundness', ',', 'ultimate', 'goal', ',', 'plot', 'number', 'distinct', 'neighbor', 'token', 'encounter', 'training', 'corpus', ',', 'rank', 'high', 'low', ',', 'Figure', '3', '.', 'rank', 'occupy', 'single', '-', 'character', 'token', 'context', '-', 'null', 'design', ',', 'SAGE', 'make', 'place', 'context', '.', 'dozen', 'token', ',', 'SAGE', 'context', 'count', 'dip', 'BPE', ',', 'trend', 'continue', 'way', 'vocabulary', ',', 'make', 'contextually', 'coherent', 'set', '.', 'finding', 'hold', 'english', 'Turkish', ',', 'replicate', 'tak-', 'ing', 'context', 'window', 'size', '2', ',', 'different', 'sage', 'construction', '.', 'figure', '4', ':', 'distribution', 'token', 'neighbor', '/', 'frequency', 'ratio', 'width-5', 'window', 'English', '(', ')', 'turkish', '(', ')', ';', 'BPE', '(', 'leave', ')', 'sage', '(', 'right', ')', '628', 'finding', 'arguably', 'attribute', 'fre-', 'quency', 'artifact', ',', 'SAGE', 'simply', 'output', 'token', 'low', 'frequency', 'order', 'provide', 'few', 'context', '.', 'present', 'nor-', 'malize', 'analysis', 'figure', '4', ',', 'depict', 'ratio', 'token', 'number', 'unique', 'neighbor', 'frequency', ',', 'distribute', 'entire', 'vocab-', 'ulary', '.', 'SAGE', 'provide', 'substantially', 'low', 'ratio', 'language', ',', 'support', 'original', 'claim', '.', '4.1', 'Robustness', 'Domain', 'change', 'possible', 'limitation', 'SAGE', 'objective', 'increase', 'reliance', 'original', 'training', 'corpus', 'compare', 'word', '-', 'count', '-', 'algorithm', '.', ',', 'necessarily', 'view', 'problem', ',', 'assume', 'collect', 'cor-', 'pus', 'faithful', 'representative', 'LLM', 'use', 'case.5', 'end', ',', 'collect', 'comparable', 'cor-', 'pora', 'non', '-', 'Wikipedia', 'domain', 'run', 'anal-', 'ysis', 'SAGE', 'BPE', 'vocabulary', 'train', 'Wikipedia', '.', 'finding', 'suggest', 'SAGE', 'lose', 'relative', 'advantage', 'context', '-', 'dependence', 'BPE', ',', 'fall', '(', 'i.e.', 'overfit', 'Wikipedia', 'domain', ')', '.', 'present', 'fertility', 'chart', 'English', 'corpus', '7.5', 'M', 'word', 'Quora', 'questions6', 'Figure', '5', ',', 'depict', 'simi-', 'lar', 'trend', 'Wikipedia', '(', 'Figure', '2', ')', 'small', 'difference', 'SAGE', 'BPE', ';', 'neighbor', '-', '-', 'frequency', 'ratio', 'aggregation', 'chart', 'Figure', '6', 'differ', 'Figure', '4', '(', ')', 'substantially', 'show', 'SAGE', 'BPE', 'token', 'di-', 'verge', 'significantly', 'measure', '.', 'repeat', 'experiment', 'english', 'legal', 'text', 'center', 'congress', 'bill', '(', 'Henderson', 'et', 'al', '.', ',', '2022', ')', '2.6', 'm', '-', 'word', 'Turkish', 'corpus', 'online', 'reviews,7', 'observe', 'similar', 'trend', '.', 'result', 'indicate', 'consider-', 'able', 'long', 'tokens', 'preferred', 'SAGE', 'select', 'optimize', 'contextuality', 'source', 'domain', ',', 'design', ',', '\"', 'short', 'blanket', '\"', 'effect', 'text', 'originate', 'different', 'domain', '.', 'wide-', 'scope', 'advantage', 'token', 'select', 'SAGE', ',', 'intrinsic', 'deficiency', 'BPE', 'long', '-', 'tail', 'token', ',', 'combination', '.', '5Indeed', ',', 'exist', 'literature', 'recommend', 'add', 'pre-', 'training', 'step', 'new', 'domain', 'fine', '-', 'tune', 'model', '(', 'e.g.', ',', 'Han', 'Eisenstein', ',', '2019', ')', '.', '6https://huggingface.co/datasets/', 'chenghao', '/', 'quora_question', '7https://huggingface.co/datasets/', 'cansen88', '/', 'turkishReviews_5_topic', 'Figure', '5', ':', 'number', 'subword', 'require', 'tokenize', 'word', 'original', 'Wikipedia', '-', 'train', 'vocabulary', ',', 'collect', 'English', 'Quora', 'question', 'corpus', '.', 'Figure', '6', ':', 'distribution', 'token', 'neighbor', '/', 'frequency', 'ratio', 'width-5', 'window', 'English', ',', 'base', 'Wikipedia', '-', 'train', 'vocabulary', 'collect', 'En-', 'glish', 'Quora', 'question', 'corpus', '.', '5', 'Downstream', 'Evaluation', 'order', 'evaluate', 'utility', 'tokenization', 'algorithm', 'major', 'NLP', 'task', ',', 'compare', 'SAGE', 'BPE', 'vocabulary', 'size', 'mean', 'pre', '-', 'train', 'BERT', '-', 'parameterized', 'model', '(', 'De-', 'vlin', 'et', 'al', '.', ',', '2019', ')', 'expedite', 'training', 'scheme', '(', 'Izsak', 'et', 'al', '.', ',', '2021', ')', '.', 'evaluate', 'LLM', 'performance', 'sequence', 'classifica-', 'tion', 'English', 'GLUE', 'benchmark', '(', 'Wang', 'et', 'al', '.', ',', '2018', ')', 'turkish', 'partition', 'XNLI', '(', 'conneau', 'et', 'al', '.', ',', '2018', ')', ',', 'name', 'entity', 'recognition', 'English', '(', 'Wang', 'et', 'al', '.', ',', '2019', ')', 'Turkish', '(', 'Al', '-', 'Rfou', 'et', 'al', '.', ',', '2015', ')', '.', 'use', 'default', 'setting', 'Huggingface', 'library', 'implementation', 'fine-', 'tuning', 'process', '(', 'Wolf', 'et', 'al', '.', ',', '2020', ')', 'perform', 'hyperparameter', 'tuning', 'model', '.', 'present', 'result', 'sequence', '-', 'level', 'task', 'Table', '4', '.', 'sage', 'tokenization', 'improve', 'perfor-', 'mance', 'nearly', 'task', 'particularly', 'sub-', '629', 'MRPC', 'MNLI', 'COLA', 'QNLI', 'SST2', 'STSB', 'QQP', 'XNLItur', '(', 'F1', ')', '(', 'acc', '%', ')', '(', 'Matt', '.', ')', '(', 'acc', '%', ')', '(', 'acc', '%', ')', '(', 'Pear', '.', ')', '(', 'acc', '%', ')', '(', 'acc', '%', ')', 'BPE', '.7918', '62.76', '.0777', '66.17', '80.54', '.3094', '82.75', '41.20', 'SAGE', '.8004', '64.00', '.0985', '74.83', '79.85', '.3387', '84.69', '46.46', 'Table', '4', ':', 'performance', 'sequence', '-', 'level', 'task', 'bert', 'model', 'train', 'different', '16k', '-', 'size', 'vocabulary', '.', 'XNLItur', 'Turkish', ',', 'rest', 'english', 'GLUE', 'task', '.', 'result', 'average', 'run', 'dev', 'set', 'different', 'seed', '.', 'English', 'turkish', 'BPE', '.7142', '.4660', 'SAGE', '.7502', '.5475', 'Table', '5', ':', 'performance', '(', 'F1', ')', 'NER', 'task', 'BERT', 'Turk-', 'ish', 'english', 'model', 'train', 'different', 'subword', 'vocabulary', 'size', '16,000', '.', 'result', 'average', 'run', 'dev', 'set', 'different', 'seed', '.', 'stantial', 'improvement', '(', '1.3–8', 'accuracy', 'point', ')', 'NLI', 'dataset', '.', 'result', 'NER', 'present', 'Table', '5', ',', 'show', 'SAGE', 'dominance', 'BPE', '.', 'length', 'training', 'pipeline', 'lead-', 'ing', 'vocabulary', 'creation', 'pre', '-', 'train', 'fine', '-', 'tuning', ',', 'difficult', 'find', 'individual', 'ex-', 'ample', 'difference', 'tokenization', 'lead', 'direct', 'change', 'prediction', ';', 'attribute', 'con-', 'sistent', 'overall', 'gain', 'downstream', 'performance', 'LLM', 'pre', '-', 'training', 'step', ',', 'design', 'SAGE', 'context', '-', 'friendly', 'vocabulary', 'en-', 'able', 'coherent', 'contextual', 'signal', 'flow', 'transformer', 'layer', 'backpropaga-', 'tion', '.', 'note', 'general', ',', 'model', 'fare', 'bad', 'GLUE', 'task', 'compare', 'Izsak', 'et', 'al', '.', '(', '2021', ')', '.', 'attribute', 'small', 'token', 'vocab-', 'ulary', 'size', ',', 'substantially', 'small', 'pre', '-', 'training', 'corpus', 'experiment', '.', '6', 'Related', 'work', 'recent', 'year', ',', 'grow', 'body', 'research', 'demonstrate', 'shortcoming', 'exist', 'tok-', 'enization', 'algorithm', 'context', 'represent-', 'ing', 'linguistic', 'phenomenon', 'different', 'language', 'different', 'task', '(', 'Banerjee', 'Bhattacharyya', ',', '2018', ';', 'Klein', 'Tsarfaty', ',', '2020', ';', 'Hakimi', 'Parizi', 'Cook', ',', '2020', ';', 'Rust', 'et', 'al', '.', ',', '2021', ';', 'Maronikolakis', 'et', 'al', '.', ',', '2021', ';', 'Mielke', 'et', 'al', '.', ',', '2021', ';', 'Hofmann', 'et', 'al', '.', ',', '2021', ')', ',', 'statistical', 'property', 'affect', 'downstream', 'performance', '(', 'Bostrom', 'Durrett', ',', '2020', ')', '.', 'work', 'address', 'concern', 'raise', 'line', 'work', 'introduce', 'improve', 'sub-', 'word', 'vocabulary', 'creation', 'method', 'leverage', 'contextual', 'aspect', 'main', 'intend', 'use', 'case', ',', 'llm', '.', 'previous', 'work', 'goal', 'include', 'algorithm', 'offer', 'robustness', 'exist', 'subword', 'vocabulary', '(', 'Provilkov', 'et', 'al', '.', ',', '2020', ';', 'et', 'al', '.', ',', '2020', ';', 'Hiraoka', ',', '2022', ')', ',', 'neces-', 'sitate', 'modification', 'training', ',', 'inference', ',', 'procedure', 'context', 'llm', '.', 'consider', 'tune', 'size', 'subword', 'vo-', 'cabulary', '(', 'Salesky', 'et', 'al', '.', ',', '2020', ')', ',', 'select', 'enlarge', 'set', 'possible', 'segmentation', '(', 'Asgari', 'et', 'al', '.', ',', '2020', ')', ',', 'optimize', 'performance', 'down-', 'stream', 'task', '.', 'alternative', 'tokenization', 'method', 'focus', 'application', 'model', 'consider', 'expect', 'downstream', 'task', 'pre-', 'training', 'corpus', '(', 'Hiraoka', 'et', 'al', '.', ',', '2020', ')', ',', 'de-', 'gree', 'jointly', 'optimize', 'tokenizer', 'downstream', 'model', '(', 'Hiraoka', 'et', 'al', '.', ',', '2021', ')', '.', 'ad-', 'dition', 'massive', 'change', 'train', 'in-', 'ference', 'procedure', 'approach', 'incur', ',', 'note', 'difficult', 'apply', 'large', 'contextualize', 'model', 'long', 'path', 'tokenization', 'prediction', ';', 'SAGE', 'overcome', 'problem', '\"', 'nudge', '\"', 'LLM', 'vocabulary', 'contextualization', '-', 'friendly', 'segmentation', '.', 'concept', 'subword', 'tokenization', 'rise', 'alongside', 'contextualize', 'representa-', 'tion', ',', 'mean', 'little', 'work', 'exist', 'skip-', 'gram', 'static', 'model', 'train', 'proper', 'subword', 'segmentation', '.', 'recently', ',', 'Kaushal', 'Mahowald', '(', '2022', ')', 'proof', '-', '-', 'concept', 'spelling', 'prediction', 'model', ',', 'lieu', 'train', 'llm', '.', 'knowledge', ',', 'work', 'date', 'static', 'embed', '-', 'base', 'objective', 'score', 'token', 'sequence', 'likelihood', 'separate', 'task', '(', 'vocabulary', 'trimming', ')', '.', 'finally', ',', 'acknowledge', 'recent', 'effort', 'away', 'tokenization', 'altogether', ',', 'character', '-', '(', 'Clark', 'et', 'al', '.', ',', '2022', ')', 'byte-', '(', 'Xue', 'et', 'al', '.', ',', '2022', ')', 'model', ',', 'encode', 'character', 'visually', 'pass', 'vi-', 'sion', 'model', '(', 'Salesky', 'et', 'al', '.', ',', '2021', ';', 'Rust', 'et', 'al', '.', ',', '2022', ')', '.', '630', 'represent', 'radical', 'departure', 'establish', 'application', 'llm', ',', 'look', 'forward', 'test', 'ability', 'im-', 'prove', 'contextual', 'subword', 'tokenization', 'method', '.', 'note', 'model', 'fac-', 'ing', 'issue', 'scale', ',', 'decod-', 'ing', ',', 'SAGE', 'vocabulary', 'ready', 'immediately', 'exist', 'popular', 'LLM', 'imple-', 'mentation', '.', 'furthermore', ',', 'recent', 'work', 'show', 'limited', 'utility', 'character', '-', 'level', 'transformer', 'semantic', 'task', ',', 'morphologically', 'rich', 'language', 'nontrivial', 'orthography', '-', 'morphology', 'relation', '(', 'Keren', 'et', 'al', '.', ',', '2022', ')', '.', '7', 'conclusion', 'work', ',', 'introduce', 'SAGE', ',', 'context', '-', 'aware', 'tokenizer', 'build', 'insight', 'BPE', ',', 'uni-', 'GRAMLM', ',', 'SKIPGRAM', ',', 'show', 'achieve', 'well', 'result', 'LLM', '-', 'pre-', 'train', '-', '-', 'fine', '-', 'tune', 'schema', 'typologically', 'distant', 'language', 'sequence', 'token', 'level', '.', 'believe', 'investigation', 'incorporate', 'context', 'tokenization', 'model', 'improve', 'result', ',', 'intend', 'extend', 'effort', 'language', 'writ-', 'ing', 'system', ',', 'multilingual', 'tokenizer', '.', 'example', ',', 'plan', 'apply', 'SAGE', 'con-', 'text', 'abjad', 'like', 'Hebrew', 'Arabic', ',', 'language', 'write', 'alphasyllabarie', 'Devanagari', '.', 'sage', ',', 'room', 'improve-', 'ment', '.', 'algorithm', 'relatively', 'slow', ',', 'take', 'roughly', 'day', 'run', 'strong', 'cpu', ',', 'make', 'dif-', 'ficult', 'apply', 'truly', 'large', 'corpus', ',', 'start', 'large', 'initial', 'vocabulary', ',', 'conduct', 'exhaus-', 'tive', 'search', 'hyperparameter', '.', 'intend', 'optimize', ',', 'continue', 'evaluation', 'subword', 'character', '-', 'schemas', '.', 'limitation', 'acknowledge', 'limitation', 'SAGE', ',', 'novel', 'algorithm', 'development', 'stage', '.', ',', 'scale', 'vocabulary', 'creation', 'framework', 'corpus', '-', 'level', 'unigram', 'statistic', 'context', 'dependence', 'incur', 'point', 'linear', 'fac-', 'tor', 'turn', 'quadratic', ',', 'bad', '.', 'introduce', 'heuristic', 'alleviate', 'issue', '§', '3', ',', 'how-', 'SAGE', 'take', 'long', 'train', 'compare', 'BPE', 'tokenizer', ',', 'roughly', 'factor', '.', 'have', 'effect', 'downstream', 'pre-', 'training', 'fine', '-', 'tune', 'step', ',', 'mean', 'hy-', 'perparameter', 'difficult', 'tune', '.', 'second', ',', 'prohibitive', 'resource', 'require', 'implement', 'LLM', 'pipeline', 'limit', 'downstream', 'evaluation', 'setup', 'individual', 'task', 'lan-', 'guage', '.', 'ideally', ',', 'language', 'di-', 'verse', 'script', 'typological', 'property', 'exam-', 'ine', ',', 'well', 'generalization', 'utility', 'integrate', 'context', 'subword', 'tokenizer', 'vocabulary', '.', 'finally', ',', 'well-', 'form', 'theory', 'integrate', 'multiple', 'domain', ',', 'lan-', 'guage', ',', 'script', 'single', 'vocabulary', '.', 'question', 'interested', 'researcher', 'recent', 'year', '(', 'e.g.', ',', 'Chung', 'et', 'al', '.', ',', '2020', ';', 'Rust', 'et', 'al', '.', ',', '2021', ';', 'Zhang', 'et', 'al', '.', ',', '2022', ')', ',', 'tokenizer', '-', 'internal', 'solu-', 'tion', '(', 'oppose', 'data', 'balance', 'manipulation', ')', 'elude', 'community', '.', 'question', 'affect', 'sage', 'tokenizer', ',', 'give', 'reliance', 'context', ',', 'change', 'starkly', 'consider', 'multiple', 'source', 'text', 'unison', '.', 'acknowledgment', 'thank', 'Jacob', 'Eisenstein', 'Cassandra', 'Jacobs', 'work', 'early', 'version', 'high', '-', 'level', 'idea', ',', 'Timo', 'Schick', 'L¨utfi', 'Kerem', 'Senel', 'fruitful', 'conversation', 'early', 'stage', 'project', '.', 'thank', 'Marco', 'Cognetta', ',', 'Michael', 'Elhadad', ',', 'Omer', 'Levy', ',', 'attendee', 'ISCOL', '2022', 'comment', 'suggestion', 'recent', 'version', 'work', '.', 'thank', 'reviewer', 'helpful', 'comment', '.', 'thank', 'Kaj', 'Bostrom', ',', 'Peter', 'Izsak', ',', 'Tamar', 'Levy', 'helping', 'obtain', 'operate', 'resource', 'training', 'model', '.', 'References', 'Rami', 'Al', '-', 'Rfou', ',', 'Vivek', 'Kulkarni', ',', 'Bryan', 'Perozzi', ',', 'Steven', 'Skiena', '.', '2015', '.', 'polyglot', '-', 'ner', ':', 'massive', 'multi-', 'lingual', 'name', 'entity', 'recognition', '.', 'proceeding', '2015', 'siam', 'International', 'Conference', 'Data', 'Mining', ',', 'page', '586–594', '.', 'siam', '.', 'Ehsaneddin', 'Asgari', ',', 'Masoud', 'Jalili', 'Sabet', ',', 'Philipp', 'Dufter', ',', 'Christopher', 'Ringlstetter', ',', 'Hinrich', 'Sch¨utze', '.', '2020', '.', 'Subword', 'sample', 'low', 'resource', 'word', 'alignment', '.', 'arXiv', 'preprint', 'arXiv:2012.11657', '.', 'Tamali', 'Banerjee', 'Pushpak', 'Bhattacharyya', '.', '2018', '.', 'Meaningless', 'meaningful', ':', 'morphology', 'ground', 'subword', '-', 'level', 'nmt', '.', 'Proceedings', 'sec-', 'ond', 'Workshop', 'Subword', '/', 'Character', 'Level', 'Models', ',', 'page', '55–60', '.', 'Balthasar', 'Bickel', 'Johanna', 'Nichols', '.', '2013a', '.', 'ex-', 'ponence', 'select', 'inflectional', 'formative', '.', 'Matthew', 'S.', 'Dryer', 'Martin', 'Haspelmath', ',', 'editor', ',', 'World', 'Atlas', 'Language', 'Structures', 'Online', '.', '631', 'Max', 'Planck', 'Institute', 'Evolutionary', 'Anthropology', ',', 'Leipzig', '.', 'Balthasar', 'Bickel', 'Johanna', 'Nichols', '.', '2013b', '.', 'Inflec-', 'tional', 'synthesis', 'verb', '.', 'Matthew', 'S.', 'Dryer', 'Martin', 'Haspelmath', ',', 'editor', ',', 'World', 'Atlas', 'Language', 'Structures', 'Online', '.', 'Max', 'Planck', 'Institute', 'Evolutionary', 'Anthropology', ',', 'Leipzig', '.', 'Kaj', 'Bostrom', 'Greg', 'Durrett', '.', '2020', '.', 'byte', 'pair', 'encod-', 'ing', 'suboptimal', 'language', 'model', 'pretraining', '.', 'Findings', 'Association', 'Computational', 'Lin-', 'guistic', ':', 'EMNLP', '2020', ',', 'page', '4617–4624', ',', 'Online', '.', 'Association', 'Computational', 'Linguistics', '.', 'Hyung', 'Won', 'Chung', ',', 'Dan', 'Garrette', ',', 'Kiat', 'Chuan', 'Tan', ',', 'Jason', 'Riesa', '.', '2020', '.', 'improve', 'multilingual', 'model', 'language', '-', 'cluster', 'vocabulary', '.', 'proceed-', 'ing', '2020', 'Conference', 'Empirical', 'Methods', 'Natural', 'Language', 'Processing', '(', 'EMNLP', ')', ',', 'page', '4536–4546', ',', 'Online', '.', 'Association', 'Computational', 'Linguistics', '.', 'Jonathan', 'H.', 'Clark', ',', 'Dan', 'Garrette', ',', 'Iulia', 'Turc', ',', 'John', 'Wieting', '.', '2022', '.', 'Canine', ':', 'pre', '-', 'training', 'efficient', 'tokenization', '-', 'free', 'encoder', 'language', 'representa-', 'tion', '.', 'Transactions', 'Association', 'Computa-', 'tional', 'Linguistics', ',', '10:73–91', '.', 'Ronan', 'Collobert', 'Jason', 'Weston', '.', '2008', '.', 'unify', 'architecture', 'natural', 'language', 'processing', ':', 'deep', 'neural', 'network', 'multitask', 'learn', '.', 'proceed-', 'ing', '25th', 'international', 'conference', 'Machine', 'learning', ',', 'page', '160–167', '.', 'Alexis', 'Conneau', ',', 'Ruty', 'Rinott', ',', 'Guillaume', 'Lample', ',', 'Adina', 'Williams', ',', 'Samuel', 'Bowman', ',', 'Holger', 'Schwenk', ',', 'Veselin', 'Stoyanov', '.', '2018', '.', 'XNLI', ':', 'evaluate', 'cross-', 'lingual', 'sentence', 'representation', '.', 'proceeding', '2018', 'Conference', 'Empirical', 'Methods', 'Nat-', 'ural', 'Language', 'Processing', ',', 'page', '2475–2485', ',', 'brus-', 'sel', ',', 'Belgium', '.', 'Association', 'Computational', 'Lin-', 'guistic', '.', 'Jacob', 'Devlin', ',', 'Ming', '-', 'Wei', 'Chang', ',', 'Kenton', 'Lee', ',', 'Kristina', 'Toutanova', '.', '2019', '.', 'BERT', ':', 'Pre', '-', 'training', 'deep', 'bidirectional', 'transformer', 'language', 'under-', 'stand', '.', 'Proceedings', '2019', 'Conference', 'North', 'american', 'Chapter', 'Association', 'Computational', 'Linguistics', ':', 'Human', 'Language', 'Tech-', 'nologie', ',', 'volume', '1', '(', 'Long', 'Short', 'Papers', ')', ',', 'page', '4171–4186', ',', 'Minneapolis', ',', 'Minnesota', '.', 'Association', 'Computational', 'Linguistics', '.', 'Philip', 'Gage', '.', '1994', '.', 'new', 'algorithm', 'data', 'compres-', 'sion', '.', 'C', 'Users', 'Journal', 'archive', ',', '12:23–38', '.', 'Ali', 'Hakimi', 'Parizi', 'Paul', 'Cook', '.', '2020', '.', 'evaluate', 'sub', '-', 'word', 'embedding', 'cross', '-', 'lingual', 'model', '.', 'Proceedings', 'Twelfth', 'Language', 'Resources', 'Evaluation', 'Conference', ',', 'page', '2712–2719', ',', 'Marseille', ',', 'France', '.', 'European', 'Language', 'Resources', 'Association', '.', 'Xiaochuang', 'Han', 'Jacob', 'Eisenstein', '.', '2019', '.', 'unsu-', 'pervise', 'domain', 'adaptation', 'contextualize', 'em-', 'bedding', 'sequence', 'labeling', '.', 'proceeding', '2019', 'Conference', 'Empirical', 'Methods', 'natu-', 'ral', 'Language', 'Processing', '9th', 'International', 'Joint', 'Conference', 'Natural', 'Language', 'Processing', '(', 'EMNLP', '-', 'IJCNLP', ')', ',', 'page', '4238–4248', ',', 'Hong', 'Kong', ',', 'China', '.', 'Association', 'Computational', 'Linguistics', '.', 'Xuanli', ',', 'Gholamreza', 'Haffari', ',', 'Mohammad', 'Norouzi', '.', '2020', '.', 'dynamic', 'programming', 'encoding', 'subword', 'segmentation', 'neural', 'machine', 'transla-', 'tion', '.', 'proceeding', '58th', 'Annual', 'Meeting', 'Association', 'Computational', 'Linguistics', ',', 'page', '3042–3051', ',', 'Online', '.', 'Association', 'Computational', 'Linguistics', '.', 'Peter', 'Henderson', ',', 'Mark', 'S.', 'Krass', ',', 'Lucia', 'Zheng', ',', 'Neel', 'Guha', ',', 'Christopher', 'D.', 'Manning', ',', 'Dan', 'Jurafsky', ',', 'Daniel', 'E.', 'Ho', '.', '2022', '.', 'pile', 'law', ':', 'learn', 'respon-', 'sible', 'datum', 'filtering', 'law', '256', 'gb', 'open-', 'source', 'legal', 'dataset', '.', 'Tatsuya', 'Hiraoka', '.', '2022', '.', 'Maxmatch', '-', 'dropout', ':', 'Sub-', 'word', 'regularization', 'wordpiece', '.', 'arXiv', 'preprint', 'arXiv:2209.04126', '.', 'Tatsuya', 'Hiraoka', ',', 'Sho', 'Takase', ',', 'Kei', 'Uchiumi', ',', 'Atsushi', 'Keyaki', ',', 'Naoaki', 'Okazaki', '.', '2020', '.', 'optimize', 'word', 'segmentation', 'downstream', 'task', '.', 'find-', 'ing', 'Association', 'Computational', 'Linguistics', ':', 'EMNLP', '2020', ',', 'page', '1341–1351', ',', 'Online', '.', 'Association', 'Computational', 'Linguistics', '.', 'Tatsuya', 'Hiraoka', ',', 'Sho', 'Takase', ',', 'Kei', 'Uchiumi', ',', 'Atsushi', 'Keyaki', ',', 'Naoaki', 'Okazaki', '.', '2021', '.', 'joint', 'optimiza-', 'tion', 'tokenization', 'downstream', 'model', '.', 'find-', 'ing', 'Association', 'Computational', 'Linguistics', ':', 'ACL', '-', 'IJCNLP', '2021', ',', 'page', '244–255', ',', 'Online', '.', 'associa-', 'tion', 'Computational', 'Linguistics', '.', 'Valentin', 'Hofmann', ',', 'Janet', 'Pierrehumbert', ',', 'Hinrich', 'Sch¨utze', '.', '2021', '.', 'superbizarre', 'superb', ':', 'deriva-', 'tional', 'morphology', 'improve', 'BERT', 'interpretation', 'complex', 'word', '.', 'proceeding', '59th', 'Annual', 'Meeting', 'Association', 'Computational', 'Lin-', 'guistic', '11th', 'International', 'Joint', 'Conference', 'Natural', 'Language', 'Processing', '(', 'volume', '1', ':', 'Long', 'Papers', ')', ',', 'page', '3594–3608', ',', 'Online', '.', 'Association', 'Computational', 'Linguistics', '.', 'Peter', 'Izsak', ',', 'Moshe', 'Berchansky', ',', 'Omer', 'Levy', '.', '2021', '.', 'train', 'BERT', 'academic', 'budget', '.', 'Pro-', 'ceeding', '2021', 'Conference', 'Empirical', 'Meth-', 'ods', 'Natural', 'Language', 'Processing', ',', 'page', '10644', '–', '10652', ',', 'Online', 'Punta', 'Cana', ',', 'Dominican', 'Republic', '.', 'Association', 'Computational', 'Linguistics', '.', 'Ayush', 'Kaushal', 'Kyle', 'Mahowald', '.', '2022', '.', 'token', 'know', 'character', 'know', '?', 'arxiv', 'preprint', 'arXiv:2206.02608', '.', 'Omri', 'Keren', ',', 'Tal', 'Avinari', ',', 'Reut', 'Tsarfaty', ',', 'Omer', 'Levy', '.', '2022', '.', 'break', 'character', ':', 'subword', 'good', 'mrl', '?', 'arXiv', 'preprint', 'arXiv:2204.04748', '.', '632', 'Stav', 'Klein', 'Reut', 'Tsarfaty', '.', '2020', '.', 'get', '#', '#', 'life', 'live', ':', 'adequate', 'word', '-', 'piece', 'mod-', 'elle', 'complex', 'morphology', '?', 'proceeding', '17th', 'SIGMORPHON', 'Workshop', 'Computational', 'Research', 'Phonetics', ',', 'Phonology', ',', 'Morphology', ',', 'page', '204–209', ',', 'Online', '.', 'Association', 'Computa-', 'tional', 'Linguistics', '.', 'Taku', 'Kudo', '.', '2018', '.', 'Subword', 'regularization', ':', 'Improv-', 'ing', 'neural', 'network', 'translation', 'model', 'multiple', 'subword', 'candidate', '.', 'proceeding', '56th', 'an-', 'nual', 'Meeting', 'Association', 'Computational', 'Linguistics', '(', 'volume', '1', ':', 'Long', 'Papers', ')', ',', 'page', '66–75', ',', 'Melbourne', ',', 'Australia', '.', 'Association', 'Computational', 'Linguistics', '.', 'Taku', 'Kudo', 'John', 'Richardson', '.', '2018', '.', 'SentencePiece', ':', 'simple', 'language', 'independent', 'subword', 'tok-', 'enizer', 'detokenizer', 'neural', 'text', 'processing', '.', 'Proceedings', '2018', 'Conference', 'Empirical', 'Methods', 'Natural', 'Language', 'processing', ':', 'system', 'Demonstrations', ',', 'page', '66–71', ',', 'Brussels', ',', 'Belgium', '.', 'Association', 'Computational', 'Linguistics', '.', 'Yinhan', 'Liu', ',', 'Myle', 'Ott', ',', 'Naman', 'Goyal', ',', 'Jingfei', 'Du', ',', 'Man-', 'dar', 'Joshi', ',', 'Danqi', 'Chen', ',', 'Omer', 'Levy', ',', 'Mike', 'Lewis', ',', 'Luke', 'Zettlemoyer', ',', 'Veselin', 'Stoyanov', '.', '2019', '.', 'Roberta', ':', 'robustly', 'optimize', 'bert', 'pretraine', 'ap-', 'proach', '.', 'arXiv', 'preprint', 'arxiv:1907.11692', '.', 'Antonis', 'Maronikolakis', ',', 'Philipp', 'Dufter', ',', 'Hinrich', 'Sch¨utze', '.', '2021', '.', 'wine', 'v', 'n.', 'compatibility', 'tokenization', 'language', '.', 'Findings', 'Association', 'Computational', 'Linguistics', ':', 'EMNLP', '2021', ',', 'page', '2382–2399', ',', 'Punta', 'Cana', ',', 'dominican', 're-', 'public', '.', 'Association', 'Computational', 'Linguistics', '.', 'Sabrina', 'J', 'Mielke', ',', 'Zaid', 'Alyafeai', ',', 'Elizabeth', 'Salesky', ',', 'Colin', 'Raffel', ',', 'Manan', 'Dey', ',', 'Matthias', 'gall´e', ',', 'Arun', 'Raja', ',', 'Chenglei', 'Si', ',', 'Wilson', 'Y', 'Lee', ',', 'Benoˆıt', 'Sagot', ',', 'et', 'al', '.', '2021', '.', 'word', 'character', ':', 'brief', 'history', 'open', '-', 'vocabulary', 'modeling', 'tokenization', 'nlp', '.', 'arXiv', 'preprint', 'arxiv:2112.10508', '.', 'Tomas', 'Mikolov', ',', 'Ilya', 'Sutskever', ',', 'Kai', 'Chen', ',', 'Greg', 'S', 'Cor-', 'rado', ',', 'Jeff', 'Dean', '.', '2013', '.', 'distribute', 'representa-', 'tion', 'word', 'phrase', 'compositionality', '.', 'advance', 'neural', 'information', 'processing', 'system', ',', '26', '.', 'Ivan', 'Provilkov', ',', 'Dmitrii', 'Emelianenko', ',', 'Elena', 'Voita', '.', '2020', '.', 'BPE', '-', 'dropout', ':', 'simple', 'effective', 'subword', 'regularization', '.', 'proceeding', '58th', 'annual', 'Meeting', 'Association', 'Computational', 'Lin-', 'guistic', ',', 'page', '1882–1892', ',', 'Online', '.', 'Association', 'Computational', 'Linguistics', '.', 'Alec', 'Radford', ',', 'Jeffrey', 'Wu', ',', 'Rewon', 'Child', ',', 'David', 'Luan', ',', 'Dario', 'Amodei', ',', 'Ilya', 'Sutskever', ',', 'et', 'al', '.', '2019', '.', 'language', 'model', 'unsupervise', 'multitask', 'learner', '.', 'OpenAI', 'blog', ',', '1(8):9', '.', 'Radim', 'Rehurek', 'Petr', 'Sojka', '.', '2011', '.', 'Gensim', '–', 'python', 'framework', 'vector', 'space', 'modelling', '.', 'NLP', 'Centre', ',', 'Faculty', 'Informatics', ',', 'Masaryk', 'University', ',', 'Brno', ',', 'Czech', 'Republic', ',', '3(2):2', '.', 'Phillip', 'Rust', ',', 'Jonas', 'F', 'Lotz', ',', 'Emanuele', 'Bugliarello', ',', 'eliz-', 'abeth', 'Salesky', ',', 'Miryam', 'de', 'Lhoneux', ',', 'Desmond', 'Elliott', '.', '2022', '.', 'language', 'modelling', 'pixel', '.', 'arXiv', 'preprint', 'arXiv:2207.06991', '.', 'Phillip', 'Rust', ',', 'Jonas', 'Pfeiffer', ',', 'Ivan', 'vuli´c', ',', 'Sebastian', 'Ruder', ',', 'Iryna', 'Gurevych', '.', '2021', '.', 'good', 'tok-', 'enizer', '?', 'monolingual', 'performance', 'multilin-', 'gual', 'language', 'model', '.', 'proceeding', '59th', 'Annual', 'Meeting', 'Association', 'Computational', 'Linguistics', '11th', 'International', 'Joint', 'Confer-', 'ence', 'Natural', 'Language', 'Processing', '(', 'volume', '1', ':', 'Long', 'Papers', ')', ',', 'page', '3118–3135', ',', 'Online', '.', 'Association', 'Computational', 'Linguistics', '.', 'Elizabeth', 'Salesky', ',', 'David', 'Etter', ',', 'Matt', 'Post', '.', '2021', '.', 'robust', 'open', '-', 'vocabulary', 'translation', 'visual', 'text', 'representation', '.', 'proceeding', '2021', 'confer-', 'ence', 'Empirical', 'Methods', 'Natural', 'Language', 'Pro-', 'cesse', ',', 'page', '7235–7252', ',', 'Online', 'Punta', 'Cana', ',', 'Dominican', 'Republic', '.', 'Association', 'Computational', 'Linguistics', '.', 'Elizabeth', 'Salesky', ',', 'Andrew', 'Runge', ',', 'Alex', 'Coda', ',', 'Jan', 'Niehues', ',', 'Graham', 'Neubig', '.', '2020', '.', 'optimize', 'segmentation', 'granularity', 'neural', 'machine', 'transla-', 'tion', '.', 'Machine', 'Translation', ',', '34(1):41–59', '.', 'Mike', 'Schuster', 'Kaisuke', 'Nakajima', '.', '2012', '.', 'japanese', 'korean', 'voice', 'search', '.', '2012', 'ieee', 'international', 'conference', 'acoustic', ',', 'speech', 'signal', 'process-', 'ing', '(', 'ICASSP', ')', ',', 'page', '5149–5152', '.', 'ieee', '.', 'Rico', 'Sennrich', ',', 'Barry', 'Haddow', ',', 'Alexandra', 'Birch', '.', '2016', '.', 'neural', 'machine', 'translation', 'rare', 'word', 'subword', 'unit', '.', 'proceeding', '54th', 'Annual', 'Meeting', 'Association', 'Computational', 'Lin-', 'guistic', '(', 'volume', '1', ':', 'Long', 'Papers', ')', ',', 'page', '1715–1725', ',', 'Berlin', ',', 'Germany', '.', 'Association', 'Computational', 'Lin-', 'guistic', '.', 'Alex', 'Wang', ',', 'Amanpreet', 'Singh', ',', 'Julian', 'Michael', ',', 'Felix', 'Hill', ',', 'Omer', 'Levy', ',', 'Samuel', 'Bowman', '.', '2018', '.', 'glue', ':', 'multi', '-', 'task', 'benchmark', 'analysis', 'platform', 'nat-', 'ural', 'language', 'understanding', '.', 'proceeding', '2018', 'EMNLP', 'Workshop', 'blackboxnlp', ':', 'analyze', 'Interpreting', 'Neural', 'Networks', 'NLP', ',', 'page', '353–355', ',', 'Brussels', ',', 'Belgium', '.', 'association', 'Com-', 'putational', 'Linguistics', '.', 'Zihan', 'Wang', ',', 'Jingbo', 'Shang', ',', 'Liyuan', 'Liu', ',', 'Lihao', 'Lu', ',', 'Ji-', 'acheng', 'Liu', ',', 'Jiawei', 'Han', '.', '2019', '.', 'CrossWeigh', ':', 'training', 'name', 'entity', 'tagger', 'imperfect', 'anno-', 'tation', '.', 'Proceedings', '2019', 'Conference', 'Empirical', 'Methods', 'Natural', 'Language', 'Processing', '9th', 'International', 'Joint', 'Conference', 'Natu-', 'ral', 'Language', 'Processing', '(', 'emnlp', '-', 'IJCNLP', ')', ',', 'page', '5154–5163', ',', 'Hong', 'Kong', ',', 'China', '.', 'association', 'Com-', 'putational', 'Linguistics', '.', 'Thomas', 'Wolf', ',', 'Lysandre', 'Debut', ',', 'Victor', 'Sanh', ',', 'Julien', 'Chaumond', ',', 'Clement', 'Delangue', ',', 'Anthony', 'Moi', ',', 'Pier-', 'ric', 'Cistac', ',', 'Tim', 'Rault', ',', 'Remi', 'Louf', ',', 'Morgan', 'Funtow-', 'icz', ',', 'Joe', 'Davison', ',', 'Sam', 'Shleifer', ',', 'Patrick', 'von', 'Platen', ',', 'Clara', 'Ma', ',', 'Yacine', 'Jernite', ',', 'Julien', 'Plu', ',', 'Canwen', 'Xu', ',', '633', 'Teven', 'Le', 'Scao', ',', 'Sylvain', 'Gugger', ',', 'Mariama', 'Drame', ',', 'Quentin', 'Lhoest', ',', 'Alexander', 'Rush', '.', '2020', '.', 'Trans-', 'former', ':', 'state', '-', '-', '-', 'art', 'natural', 'language', 'processing', '.', 'Proceedings', '2020', 'Conference', 'Empirical', 'Methods', 'Natural', 'Language', 'processing', ':', 'system', 'Demonstrations', ',', 'page', '38–45', ',', 'Online', '.', 'Association', 'Computational', 'Linguistics', '.', 'Yonghui', 'Wu', ',', 'Mike', 'Schuster', ',', 'Zhifeng', 'Chen', ',', 'Quoc', 'V.', 'Le', ',', 'Mohammad', 'Norouzi', ',', 'Wolfgang', 'Macherey', ',', 'Maxim', 'Krikun', ',', 'Yuan', 'Cao', ',', 'Qin', 'Gao', ',', 'Klaus', 'Macherey', ',', 'Jeff', 'Klingner', ',', 'Apurva', 'Shah', ',', 'Melvin', 'Johnson', ',', 'Xiaobing', 'Liu', ',', 'Łukasz', 'Kaiser', ',', 'Stephan', 'Gouws', ',', 'Yoshikiyo', 'Kato', ',', 'Taku', 'Kudo', ',', 'Hideto', 'Kazawa', ',', 'Keith', 'Stevens', ',', 'George', 'Kurian', ',', 'Nishant', 'Patil', ',', 'Wei', 'Wang', ',', 'Cliff', 'Young', ',', 'Jason', 'Smith', ',', 'Jason', 'Riesa', ',', 'Alex', 'Rudnick', ',', 'Oriol', 'Vinyals', ',', 'Greg', 'Corrado', ',', 'Macduff', 'Hughes', ',', 'Jeffrey', 'Dean', '.', '2016', '.', 'Google', 'neural', 'machine', 'translation', 'system', ':', 'bridging', 'gap', 'human', 'machine', 'trans-', 'lation', '.', 'arXiv', 'preprint', 'arxiv:1609.08144', '.', 'linte', 'Xue', ',', 'Aditya', 'Barua', ',', 'Noah', 'Constant', ',', 'Rami', 'al-', 'Rfou', ',', 'Sharan', 'Narang', ',', 'Mihir', 'Kale', ',', 'Adam', 'Roberts', ',', 'Colin', 'Raffel', '.', '2022', '.', 'byt5', ':', 'token', '-', 'free', 'future', 'pre', '-', 'train', 'byte', '-', '-', 'byte', 'model', '.', 'Transac-', 'tion', 'Association', 'Computational', 'Linguis-', 'tic', ',', '10:291–306', '.', 'Shiyue', 'Zhang', ',', 'Vishrav', 'Chaudhary', ',', 'Naman', 'Goyal', ',', 'James', 'Cross', ',', 'Guillaume', 'Wenzek', ',', 'Mohit', 'Bansal', ',', 'Francisco', 'Guzman', '.', '2022', '.', 'robust', 'neural', 'ma-', 'chine', 'translation', 'language', 'imbalance', 'multilin-', 'gual', 'tokenizer', 'training', '?', 'proceeding', '15th', 'biennial', 'conference', 'Association', 'Machine', 'Translation', 'Americas', '(', 'volume', '1', ':', 'Research', 'Track', ')', ',', 'page', '97–116', ',', 'Orlando', ',', 'USA', '.', 'Association', 'Machine', 'Translation', 'Americas', '.', '634', 'Final', 'Vocab', 'Size', '16', 'K', 'Initial', 'Vocab', 'Size', '20', 'K', 'k', '(', 'tokens', 'prune', 'batch', ')', '100', 'M', '(', 'size', 'pruning', 'candidate', 'set', ')', '1500', 'm', '(', 'likelihood', 'recalculation', 'frequency', ')', '10', 'l', '(', 'embed', 'recalculation', 'frequency', ')', '4', 'SAGE', 'window', 'size', '5', 'word2vec', 'window', 'size', '5', 'Word2Vec', 'vector', 'dimension', '50', 'Word2Vec', 'negative', 'sample', '15', 'Table', '6', ':', 'hyperparameter', 'vocabulary', 'creation', '.', 'Hyperparameters', 'Table', '6', ',', '7', ',', '8', ',', 'present', 'hyperparame-', 'ter', 'training', 'element', 'experiment', '.', 'B', 'Computing', 'Resources', 'experiment', 'Quadro', 'RTX', '8000', 'GPU', '.', 'layer', 'norm', 'type', 'pytorch', 'model', 'type', 'bert', '-', 'mlm', 'hide', 'act', 'gelu', 'hide', 'size', '1024', 'num', 'hide', 'layer', '24', 'num', 'attention', 'head', '16', 'intermediate', 'size', '4096', 'hide', 'dropout', 'prob', '0.1', 'attention', 'prob', 'dropout', 'prob', '0.1', 'encoder', 'ln', 'mode', 'pre', '-', 'ln', 'lr', '1e-3', 'train', 'batch', 'size', '4032', 'train', 'micro', 'batch', 'size', 'gpu', '32', 'lr', 'schedule', 'time', 'curve', 'linear', 'warmup', 'proportion', '0.06', 'gradient', 'clip', '0.0', 'optimizer', 'type', 'adamw', 'weight', 'decay', '0.01', 'adam', 'beta1', '0.9', 'adam', 'beta2', '0.98', 'adam', 'eps', '1e-6', 'total', 'training', 'time', '24.0', 'optimizer', 'type', 'adamw', 'validation', 'epoch', '3', 'validation', 'epoch', 'begin', '1', 'validation', 'epoch', 'end', '1', 'validation', 'begin', 'proportion', '0.05', 'validation', 'end', 'proportion', '0.01', 'validation', 'micro', 'batch', '16', 'deepspeed', 'yes', 'datum', 'loader', 'type', 'dist', 'Table', '7', ':', 'Hyperparameters', 'pre', '-', 'train', 'bert-', 'architecture', 'model', 'academic', '-', 'budget', '-', 'bert', 'code', '(', 'Izsak', 'et', 'al', '.', ',', '2021', ')', '.', 'max', 'seq', 'length', '128', 'evaluation', 'strategy', 'step', 'device', 'train', 'batch', 'size', '16', 'gradient', 'accumulation', 'step', '1', 'device', 'eval', 'batch', 'size', '16', 'learning', 'rate', '5e-5', 'weight', 'decay', '0.1', 'max', 'grad', 'norm', '1.0', 'lr', 'scheduler', 'type', 'polynomial', 'warmup', 'step', '50', 'Table', '8', ':', 'hyperparameter', 'fine', '-', 'tune', 'task', 'script', 'academic', '-', 'budget', '-', 'bert', 'package', '.', '635']\n"
     ]
    }
   ],
   "source": [
    "texts = load_documents(files_list)\n",
    "texts = remove_stopwords(texts)\n",
    "stats = get_doc_statistics(texts)\n",
    "\n",
    "# Exibe as estatísticas\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentences_per_doc</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_tokens_per_doc</th>\n",
       "      <th>top_10_tokens</th>\n",
       "      <th>down_10_tokens</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_prepositions</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>457</td>\n",
       "      <td>457.0</td>\n",
       "      <td>4405</td>\n",
       "      <td>4405.0</td>\n",
       "      <td>[(vocabulary, 73), (sage, 71), (tokens, 52), (...</td>\n",
       "      <td>[(loader, 1), (dist, 1), (seq, 1), (strategy, ...</td>\n",
       "      <td>1691</td>\n",
       "      <td>653</td>\n",
       "      <td>31</td>\n",
       "      <td>[Proceedings, 17th, Conference, European, Chap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_sentences  avg_sentences_per_doc  num_tokens  avg_tokens_per_doc  \\\n",
       "0            457                  457.0        4405              4405.0   \n",
       "\n",
       "                                       top_10_tokens  \\\n",
       "0  [(vocabulary, 73), (sage, 71), (tokens, 52), (...   \n",
       "\n",
       "                                      down_10_tokens  num_nouns  num_verbs  \\\n",
       "0  [(loader, 1), (dist, 1), (seq, 1), (strategy, ...       1691        653   \n",
       "\n",
       "   num_prepositions                                             lemmas  \n",
       "0                31  [Proceedings, 17th, Conference, European, Chap...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_stats = pd.DataFrame([stats])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
