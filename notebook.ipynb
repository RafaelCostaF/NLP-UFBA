{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Métricas textuais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de arquivos: ['documents\\\\A9_-_EACL23_Incorporating_context_into_subword_vocabularies.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"documents\"\n",
    "files_list = [os.path.join(path, file_name) for file_name in os.listdir(path)]\n",
    "\n",
    "print(\"Lista de arquivos:\", files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição das funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install pymupdf spacy chardet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import chardet\n",
    "\n",
    "# Carrega o modelo de NLP para português\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            for page in pdf:\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler PDF {file_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def load_documents(files):\n",
    "    return [extract_text_from_pdf(file_path) for file_path in files]\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc if not token.is_stop]\n",
    "        cleaned_texts.append(\" \".join(tokens))\n",
    "    return cleaned_texts\n",
    "\n",
    "def count_sentences(doc):\n",
    "    return len(list(doc.sents))\n",
    "\n",
    "def count_tokens(doc):\n",
    "    return [token.text.lower() for token in doc if token.is_alpha]\n",
    "\n",
    "def count_pos_tags(doc):\n",
    "    # Substantivos\n",
    "    num_nouns = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
    "    # Verbos\n",
    "    num_verbs = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "    # Preposições\n",
    "    num_adpositions = sum(1 for token in doc if token.pos_ == \"ADP\")\n",
    "    return num_nouns, num_verbs, num_adpositions\n",
    "\n",
    "def compute_token_stats(tokens, num_docs):\n",
    "    total_tokens = len(tokens)\n",
    "    avg_tokens = total_tokens / num_docs if num_docs else 0\n",
    "    token_freq = Counter(tokens)\n",
    "    top_10 = token_freq.most_common(10)\n",
    "    down_10 = token_freq.most_common()[-10:]\n",
    "    return total_tokens, avg_tokens, top_10, down_10\n",
    "\n",
    "def get_doc_statistics(texts):\n",
    "    total_sentences = 0\n",
    "    total_tokens_list = []\n",
    "    total_nouns = total_verbs = total_preps = 0\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "\n",
    "        total_sentences += count_sentences(doc)\n",
    "\n",
    "        tokens = count_tokens(doc)\n",
    "        total_tokens_list.extend(tokens)\n",
    "\n",
    "        nouns, verbs, preps = count_pos_tags(doc)\n",
    "        total_nouns += nouns\n",
    "        total_verbs += verbs\n",
    "        total_preps += preps\n",
    "\n",
    "    num_docs = len(texts)\n",
    "    avg_sentences = total_sentences / num_docs if num_docs else 0\n",
    "    total_tokens, avg_tokens, top_10, down_10 = compute_token_stats(total_tokens_list, num_docs)\n",
    "\n",
    "    return {\n",
    "        \"num_sentences\": total_sentences,\n",
    "        \"avg_sentences_per_doc\": avg_sentences,\n",
    "        \"num_tokens\": total_tokens,\n",
    "        \"avg_tokens_per_doc\": avg_tokens,\n",
    "        \"top_10_tokens\": top_10,\n",
    "        \"down_10_tokens\": down_10,\n",
    "        \"num_nouns\": total_nouns,\n",
    "        \"num_verbs\": total_verbs,\n",
    "        \"num_prepositions\": total_preps\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentences: 380\n",
      "avg_sentences_per_doc: 380.0\n",
      "num_tokens: 6845\n",
      "avg_tokens_per_doc: 6845.0\n",
      "top_10_tokens: [('the', 281), ('of', 207), ('and', 177), ('in', 172), ('for', 125), ('a', 122), ('to', 121), ('vocabulary', 73), ('on', 71), ('sage', 71)]\n",
      "down_10_tokens: [('loader', 1), ('dist', 1), ('seq', 1), ('strategy', 1), ('accumulation', 1), ('eval', 1), ('rate', 1), ('grad', 1), ('scheduler', 1), ('polynomial', 1)]\n",
      "num_nouns: 1822\n",
      "num_verbs: 699\n",
      "num_prepositions: 904\n"
     ]
    }
   ],
   "source": [
    "texts = load_documents(files_list)\n",
    "stats = get_doc_statistics(texts)\n",
    "\n",
    "# Exibe as estatísticas\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentences_per_doc</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_tokens_per_doc</th>\n",
       "      <th>top_10_tokens</th>\n",
       "      <th>down_10_tokens</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_prepositions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>380</td>\n",
       "      <td>380.0</td>\n",
       "      <td>6845</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>[(the, 281), (of, 207), (and, 177), (in, 172),...</td>\n",
       "      <td>[(loader, 1), (dist, 1), (seq, 1), (strategy, ...</td>\n",
       "      <td>1822</td>\n",
       "      <td>699</td>\n",
       "      <td>904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_sentences  avg_sentences_per_doc  num_tokens  avg_tokens_per_doc  \\\n",
       "0            380                  380.0        6845              6845.0   \n",
       "\n",
       "                                       top_10_tokens  \\\n",
       "0  [(the, 281), (of, 207), (and, 177), (in, 172),...   \n",
       "\n",
       "                                      down_10_tokens  num_nouns  num_verbs  \\\n",
       "0  [(loader, 1), (dist, 1), (seq, 1), (strategy, ...       1822        699   \n",
       "\n",
       "   num_prepositions  \n",
       "0               904  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_stats = pd.DataFrame([stats])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentences: 457\n",
      "avg_sentences_per_doc: 457.0\n",
      "num_tokens: 4405\n",
      "avg_tokens_per_doc: 4405.0\n",
      "top_10_tokens: [('vocabulary', 73), ('sage', 71), ('tokens', 52), ('bpe', 50), ('al', 44), ('et', 42), ('association', 38), ('token', 38), ('size', 37), ('v', 35)]\n",
      "down_10_tokens: [('loader', 1), ('dist', 1), ('seq', 1), ('strategy', 1), ('accumulation', 1), ('eval', 1), ('rate', 1), ('grad', 1), ('scheduler', 1), ('polynomial', 1)]\n",
      "num_nouns: 1691\n",
      "num_verbs: 653\n",
      "num_prepositions: 31\n"
     ]
    }
   ],
   "source": [
    "texts = load_documents(files_list)\n",
    "texts = remove_stopwords(texts)\n",
    "stats = get_doc_statistics(texts)\n",
    "\n",
    "# Exibe as estatísticas\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sentences_per_doc</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_tokens_per_doc</th>\n",
       "      <th>top_10_tokens</th>\n",
       "      <th>down_10_tokens</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_prepositions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>457</td>\n",
       "      <td>457.0</td>\n",
       "      <td>4405</td>\n",
       "      <td>4405.0</td>\n",
       "      <td>[(vocabulary, 73), (sage, 71), (tokens, 52), (...</td>\n",
       "      <td>[(loader, 1), (dist, 1), (seq, 1), (strategy, ...</td>\n",
       "      <td>1691</td>\n",
       "      <td>653</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_sentences  avg_sentences_per_doc  num_tokens  avg_tokens_per_doc  \\\n",
       "0            457                  457.0        4405              4405.0   \n",
       "\n",
       "                                       top_10_tokens  \\\n",
       "0  [(vocabulary, 73), (sage, 71), (tokens, 52), (...   \n",
       "\n",
       "                                      down_10_tokens  num_nouns  num_verbs  \\\n",
       "0  [(loader, 1), (dist, 1), (seq, 1), (strategy, ...       1691        653   \n",
       "\n",
       "   num_prepositions  \n",
       "0                31  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_stats = pd.DataFrame([stats])\n",
    "df_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
